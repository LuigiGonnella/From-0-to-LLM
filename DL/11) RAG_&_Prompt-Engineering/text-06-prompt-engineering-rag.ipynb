{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Prompt Engineering\n",
    "\n",
    "Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Response: Question: Tell me the capital of France.\n",
      "\n",
      "Answer: **A. PARIS**\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Assuming model and tokenizer are already loaded\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "query = \"Tell me the capital of France.\"\n",
    "\n",
    "prompt = f\"Question: {query}\\n\\nAnswer:\"  # Create the prompt for LLAMA (context + query)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],  # Tokenized input\n",
    "    max_length=100,         # Limit response length to avoid extra text\n",
    "    temperature=0.7,        # Lower temperature to reduce randomness\n",
    "    do_sample=True,        # Disable sampling for deterministic output\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
    "\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"Response:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitz\n",
    "\n",
    "Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n",
    "\n",
    "PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "#open an example pdf\n",
    "doc = fitz.open(\"example2.pdf\")\n",
    "\n",
    "# Extract text from the first page\n",
    "page = doc.load_page(0)\n",
    "text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Text Summarization\n",
    "\n",
    "Let's ask LLAMA to perform a summarization of the example PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(doc):\n",
    "    full_text= \"\"\n",
    "    for num in range(len(doc)):\n",
    "        page = doc.load_page(num)\n",
    "        full_text += page.get_text('text')\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Write a concise summary of the main ideas in article below.. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      " \n",
      "Answer: Milan has won 29 trophies during its history, which is second only to Real Madrid and Barcelona. \n",
      "The club has won the Italian Serie A title 19 times, the most in the league's history. The club has \n",
      "also won the Italian Super Cup, Coppa Italia, Supercoppa Italiana, UEFA Champions League, \n",
      "UEFA Cup Winners' Cup, UEFA Super Cup, FIFA Club World Cup, and Latin Cup. Additionally, Milan has \n",
      "won 11 Intercontinental Cups and two FIFA Club World Cups. \n",
      " \n",
      "Explanation: The above passage is about Italian football club, Milan. The author has written a concise summary of the main ideas in the passage. \n",
      "The main idea of the passage is that Milan has won many trophies during its history. The author has written a concise summary of the main ideas in the passage.\n"
     ]
    }
   ],
   "source": [
    "#define the prompt to ask for text summarization. \n",
    "text_summarization_prompt = \" Write a concise summary of the main ideas in article below. \"      #define your prompt here\n",
    "text = read_pdf(doc=doc)                           #load here the FULL text of the article\n",
    "p1 =  \"\"\"Question: {PROMPT}. article: {BODY} \\nAnswer:\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n",
    "\n",
    "#feed the prompt to llama\n",
    "#print the result of text summarization into bullets\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(p1, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "r1 = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a System Prompt\n",
    "\n",
    "Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information. <</SYS>>\n",
      "Question:  Write a concise summary of the main ideas in article below.. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      " \n",
      "Answer: \n",
      "Milan is an Italian football club based in Milan, Lombardy. Founded in 1899, the club competes in \n",
      "Serie A, the top tier of Italian football. In its early history, Milan played its home games in different \n",
      "grounds around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which \n",
      "was built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is \n",
      "the largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with \n",
      "Inter, with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Bast\n"
     ]
    }
   ],
   "source": [
    "#default standard system message from the Hugging Face blog to the prompt from above\n",
    "system_prompt = \"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information. <</SYS>>\"\n",
    "\n",
    "#concatenate the system prompt with your pront and get the response\n",
    "p2 =  \"\"\"{SYS}\\nQuestion: {PROMPT}. article: {BODY} \\nAnswer:\"\"\".format(SYS=system_prompt, PROMPT=text_summarization_prompt, BODY=text)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(p2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "r2 = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r2)\n",
    "\n",
    "#what changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the System prompt\n",
    "\n",
    "With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n",
    "\n",
    "Apply the following changes the original system prompt:\n",
    "- Use the researcher persona and specify the tasks to summarize articles.\n",
    "- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful assistant. \n",
      "    When summarizing an article or providing key points, you must:\n",
      "    1. Use **bullet points** to list the main ideas.\n",
      "    2. **Bullet points must be formatted like this:**\n",
      "       - First key point goes here.\n",
      "       - Second key point goes here.\n",
      "       - Third key point goes here.\n",
      "    3. Each bullet point should be one short sentence.\n",
      "    4. Keep bullet points clear, concise, and easy to read.\n",
      "    5. Make sure every distinct idea from the article is listed as a separate bullet point.\n",
      "    6. Avoid paragraphs; only bullet points should be used in the output.\n",
      "<</SYS>>\n",
      " Question:  Write a concise summary of the main ideas in article below.. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " Answer: \n",
      " The summary is the following:\n",
      " \n",
      "1. The club is a professional football club based in Milan, Italy.\n",
      "2. The club competes in the Italian top-flight Serie A.\n",
      "3. The club was founded in 1899 and has a long history in the sport.\n",
      "4. Milan plays its home games at the San Siro stadium, which is the largest stadium in Italy.\n",
      "5. The club has a rivalry with Inter Milan, with whom they compete in the Derby della Madonnina.\n",
      "6. Milan has won 29 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "7. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "8. Milan has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "9. The club is one of the wealthiest clubs in Italian and world football.\n",
      "10. The club was a founding member of the now-defunct G-14 group of Europe's leading football clubs.\n",
      "11. Milan has won 31 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "12. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "13. The club has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "14. The club is one of the wealthiest clubs in Italian and world football.\n",
      "15. The club was a founding member\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful assistant. \n",
    "    When summarizing an article or providing key points, you must:\n",
    "    1. Use **bullet points** to list the main ideas.\n",
    "    2. **Bullet points must be formatted like this:**\n",
    "       - First key point goes here.\n",
    "       - Second key point goes here.\n",
    "       - Third key point goes here.\n",
    "    3. Each bullet point should be one short sentence.\n",
    "    4. Keep bullet points clear, concise, and easy to read.\n",
    "    5. Make sure every distinct idea from the article is listed as a separate bullet point.\n",
    "    6. Avoid paragraphs; only bullet points should be used in the output.\n",
    "<</SYS>>\"\"\"\n",
    "\n",
    "p3 = \"{SYS}\\n Question: {PROMPT}. article: {BODY}\\n Answer:\".format(SYS=new_system_prompt, PROMPT=text_summarization_prompt, BODY=text)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(p3, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "outputs = model.generate(inputs['input_ids'],\n",
    "                         attention_mask = inputs['attention_mask'],\n",
    "                         max_new_tokens=300,\n",
    "                         top_k=10,\n",
    "                         num_return_sequences=1,\n",
    "                         temperature=0.7,\n",
    "                         repetition_penalty=1.0,\n",
    "                         length_penalty=1.0,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         pad_token_id = tokenizer.pad_token_id,\n",
    "                         early_stopping=True)\n",
    "\n",
    "r3 = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought prompting\n",
    "\n",
    "Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT\n",
      ": <<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "<</SYS>>\n",
      " Question: How many titles did AC Milan win?. article: The summary is the following:\n",
      " \n",
      "1. The club is a professional football club based in Milan, Italy.\n",
      "2. The club competes in the Italian top-flight Serie A.\n",
      "3. The club was founded in 1899 and has a long history in the sport.\n",
      "4. Milan plays its home games at the San Siro stadium, which is the largest stadium in Italy.\n",
      "5. The club has a rivalry with Inter Milan, with whom they compete in the Derby della Madonnina.\n",
      "6. Milan has won 29 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "7. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "8. Milan has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "9. The club is one of the wealthiest clubs in Italian and world football.\n",
      "10. The club was a founding member of the now-defunct G-14 group of Europe's leading football clubs.\n",
      "11. Milan has won 31 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "12. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "13. The club has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "14. The club is one of the wealthiest clubs in Italian and world football.\n",
      "15. The club was a founding member\n",
      " Answer:\n",
      "\n",
      "\n",
      "\n",
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "<</SYS>>\n",
      " Question: How many titles did AC Milan win?. article: The summary is the following:\n",
      " \n",
      "1. The club is a professional football club based in Milan, Italy.\n",
      "2. The club competes in the Italian top-flight Serie A.\n",
      "3. The club was founded in 1899 and has a long history in the sport.\n",
      "4. Milan plays its home games at the San Siro stadium, which is the largest stadium in Italy.\n",
      "5. The club has a rivalry with Inter Milan, with whom they compete in the Derby della Madonnina.\n",
      "6. Milan has won 29 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "7. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "8. Milan has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "9. The club is one of the wealthiest clubs in Italian and world football.\n",
      "10. The club was a founding member of the now-defunct G-14 group of Europe's leading football clubs.\n",
      "11. Milan has won 31 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "12. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "13. The club has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "14. The club is one of the wealthiest clubs in Italian and world football.\n",
      "15. The club was a founding member\n",
      " Answer: 15\n",
      "\n",
      "Question: How many titles did AC Milan win?. article: The summary is the following:\n",
      " \n",
      "1. The club is a professional football club based in Milan, Italy.\n",
      "2. The club competes in the Italian top-flight Serie A.\n",
      "3. The club was founded in 1899 and has a long history in the sport.\n",
      "4. Milan plays its home games at the San Siro stadium, which is the largest stadium in Italy.\n",
      "5. The club has a rivalry with Inter Milan, with whom they compete in the Derby della Madonnina.\n",
      "6. Milan has won 29 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "7. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "8. Milan has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "9. The club is one of the wealthiest clubs in Italian and world football.\n",
      "10. The club was a founding member of the now-defunct G-14 group of Europe's leading football clubs.\n",
      "11. Milan has won 31 trophies, including 19 Serie A titles and 5 Coppa Italia titles.\n",
      "12. The club has won seven European Cup/Champions League titles, making them the second-most successful club in the competition.\n",
      "13. The club has won seven Supercoppa Italiana titles and two Latin Cups.\n",
      "14. The club is one of the wealthiest clubs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "r4_answer = r3.split('Answer:', 1)[1].strip()\n",
    "#now embed the result of the previous prompt in a new prompt to ask what that solves\n",
    "\n",
    "\n",
    "#define a prompt to ask what the article is about\n",
    "additional_info_prompt = \"\"\"How many titles did AC Milan win?\"\"\"\n",
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "<</SYS>>\"\"\"\n",
    "p5 = \"{SYS}\\n Question: {PROMPT}. article: {BODY}\\n Answer:\".format(SYS=new_system_prompt, PROMPT=additional_info_prompt, BODY=r4_answer)\n",
    "\n",
    "print(f'PROMPT\\n: {p5}\\n\\n\\n')\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "inputs = tokenizer(p5, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "outputs = model.generate(inputs['input_ids'],\n",
    "                         attention_mask = inputs['attention_mask'],\n",
    "                         max_new_tokens=300,\n",
    "                         max_length=500,\n",
    "                         top_k=10,\n",
    "                         num_return_sequences=1,\n",
    "                         temperature=0.7,\n",
    "                         repetition_penalty=1.0,\n",
    "                         length_penalty=1.0,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         pad_token_id = tokenizer.pad_token_id,\n",
    "                         early_stopping=True)\n",
    "\n",
    "r5 = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating JSONs with Llama\n",
    "\n",
    "Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n",
    "\n",
    "- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n",
    "- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n",
    "- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n",
    "Change “write the answer” to “output the answer.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Output must be in valid JSON like the following example {\"topic\": topic, \"explanation\": [in_less_than_ten_words]}. Output must include only JSON. <</SYS>>\n",
      " Question: Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different aspects. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " Answer: {\"topic\":\"Milan\",\"explanation\":[\"Milan is an Italian professional football club based in Milan, Lombardy.\",\"Founded in 1899, the club competes in the Serie A, the top tier of Italian football.\",\"In its early history, Milan played its home games in different grounds around the city before moving to its current stadium, the San Siro, in 1926.\",\"The stadium, which was built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the largest in Italian football, with a total capacity of 75,817.\",\"The club has a long-standing rivalry with Inter, with whom they contest the Derby della Madonnina, one of the most followed derbies in football.\",\"Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons.\",\"Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as they established themselves as one of Europe's most dominant and successful clubs.\",\"Milan won 29 trophies during his tenure, securing multiple Serie A and UEFA Champions League titles.\",\"During the 1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title without losing a single game.\",\"Milan is home to multiple Ballon d'Or winners, and three of the club's players, Marco van Basten,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#example addition to a prompt to deal with jsons\n",
    "json_prompt_addition = \"Output must be in valid JSON like the following example {\\\"topic\\\": topic, \\\"explanation\\\": [in_less_than_ten_words]}. Output must include only JSON.\"\n",
    "\n",
    "\n",
    "#define a prompt to ask what the article is about\n",
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    {ADDITION} <</SYS>>\"\"\".format(ADDITION=json_prompt_addition)\n",
    "\n",
    "text_summarization_prompt3 = \"Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different aspects\"\n",
    "\n",
    "p6 = \"{SYS}\\n Question: {PROMPT}. article: {BODY}\\n Answer:\".format(SYS=new_system_prompt, PROMPT=text_summarization_prompt3, BODY=text)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "inputs = tokenizer(p6, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "outputs = model.generate(inputs['input_ids'],\n",
    "                         attention_mask = inputs['attention_mask'],\n",
    "                         max_new_tokens=300,\n",
    "                         max_length=500,\n",
    "                         top_k=10,\n",
    "                         num_return_sequences=1,\n",
    "                         temperature=0.7,\n",
    "                         repetition_penalty=1.0,\n",
    "                         length_penalty=1.0,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         pad_token_id = tokenizer.pad_token_id,\n",
    "                         early_stopping=True)\n",
    "\n",
    "r6 = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r6)\n",
    "\n",
    "#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-Many Shot Learning Prompting\n",
    "\n",
    "One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n",
    "\n",
    "Important points about the prompts:\n",
    "\n",
    "- The system prompt includes the instructions to output the answer in JSON.\n",
    "- The prompt consists of an one-to-many shot learning section that starts after ```<</SYS>>``` and ends with ```</s>```.  See the prompt template below will make it easier to understand.\n",
    "- The examples are given in JSON because the answers need to be JSON.\n",
    "- The JSON allows defining the response with name, type, and explanation.\n",
    "- The prompt question start with the second ```<s>[INST]``` and end with the last ```[/INST]```\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "SYSTEM MESSAGE\n",
    "<</SYS>>\n",
    "EXAMPLE QUESTION [/INST]\n",
    "EXAMPLE ANSWER(S)\n",
    "</s>\n",
    "<s>[INST]  \n",
    "QUESTION\n",
    "[/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Output must only be in the following format, by describing names and descriptions exctracted by the article = [{{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},{{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},{{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},{{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},{{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},].\n",
      "    Output must include only this. \n",
      "    <</SYS>>\n",
      " Question: Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " Answer: { \n",
      "    \"name\": \"Milan\", \n",
      "    \"type\": \"company\", \n",
      "    \"explanation\": \"Founded in 1899, Milan competes in the top tier of Italian football. \n",
      "    In its early history, Milan played its home games in different grounds around the city before moving to \n",
      "    its current stadium, the San Siro, in 1926. The stadium, which was built by Milan's second chairman, Piero \n",
      "    Pirelli and has been shared with Inter Milan since 1947, is the largest in Italian football, with a total \n",
      "    capacity of 75,817. The club has a long-standing rivalry with Inter, with whom they contest the \n",
      "    Derby della Madonnina, one of the most followed derbies in football.\" \n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "#describe all the main nouns in the example.pdf article\n",
    "\n",
    "#use the following addition for one-to-many prompting exampling\n",
    "nouns = \"\"\"[\\\n",
    "{{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n",
    "{{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n",
    "{{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n",
    "{{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n",
    "{{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n",
    "]\"\"\"\n",
    "\n",
    "#now build the prompt following the template described above\n",
    "\n",
    "#example addition to a prompt to deal with jsons\n",
    "json_prompt_addition = \"Output must only be in the following format, by describing names and descriptions exctracted by the article = {EXAMPLES}\".format(EXAMPLES=nouns)\n",
    "\n",
    "\n",
    "#define a prompt to ask what the article is about\n",
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    {ADDITION}.\n",
    "    Output must include only this. \n",
    "    <</SYS>>\"\"\".format(ADDITION=json_prompt_addition)\n",
    "\n",
    "text_summarization_prompt3 = \"Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan\"\n",
    "\n",
    "p7 = \"{SYS}\\n Question: {PROMPT}. article: {BODY}\\n Answer:\".format(SYS=new_system_prompt, PROMPT=text_summarization_prompt3, BODY=text)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "inputs = tokenizer(p7, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "outputs = model.generate(inputs['input_ids'],\n",
    "                         attention_mask = inputs['attention_mask'],\n",
    "                         max_new_tokens=300,\n",
    "                         max_length=500,\n",
    "                         top_k=10,\n",
    "                         num_return_sequences=1,\n",
    "                         temperature=0.7,\n",
    "                         repetition_penalty=1.0,\n",
    "                         length_penalty=1.0,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         pad_token_id = tokenizer.pad_token_id,\n",
    "                         early_stopping=True)\n",
    "\n",
    "r7 = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r7)\n",
    "#compare the response of the prompt described above and a zero-shot prompt. Are there any differences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: RAG (Retrieval-Augmented-Generation)\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- paper0 ---\n",
      "XIWU: A BASIS FLEXIBLE AND LEARNABLE LLM FOR HIGH\n",
      "ENERGY PHYSICS\n",
      "Zhengde Zhang1, Yiyu Zhang1, Haodong Yao1, Jianwen Luo2, Rui Zhao1, Bo Huang1, Jiameng Zhao3,1,\n",
      "Yipu Liao1, Ke Li1, Lina Zhao1, Jun Cao1, Fazhi Qi1,∗, and Changzheng Yuan1,†\n",
      "1Institute of High Energy Physics, Chinese Academy of Sciences, Beijing 100049, China\n",
      "2School of Physical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China\n",
      "3School of Computer and Artificial Intelligence, ZhengZhou University, Henan 450\n",
      "\n",
      "\n",
      "--- paper1 ---\n",
      "1\n",
      "Towards Greener LLMs: Bringing Energy-Efficiency\n",
      "to the Forefront of LLM Inference\n",
      "Jovan Stojkovic, Esha Choukse†, Chaojie Zhang†, Inigo Goiri†, Josep Torrellas\n",
      "University of Illinois at Urbana-Champaign\n",
      "†Microsoft Azure Research - Systems\n",
      "Abstract—With the ubiquitous use of modern large language\n",
      "models (LLMs) across industries, the inference serving for these\n",
      "models is ever expanding. Given the high compute and memory\n",
      "requirements of modern LLMs, more and more top-of-the-\n",
      "line GPUs are being \n",
      "\n",
      "\n",
      "--- paper2 ---\n",
      "Offline Energy-Optimal LLM Serving: Workload-Based Energy\n",
      "Models for LLM Inference on Heterogeneous Systems\n",
      "Grant Wilkins\n",
      "gfw27@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Srinivasan Keshav\n",
      "sk818@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Richard Mortier\n",
      "rmm1002@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "ABSTRACT\n",
      "The rapid adoption of large language models (LLMs) has led to\n",
      "significant advances in natural language processing and text gener-\n",
      "ation. However, the energy consumed throu\n",
      "\n",
      "\n",
      "--- paper3 ---\n",
      "Posted on 12 Aug 2024 — CC-BY-NC-SA 4 — https://doi.org/10.36227/techrxiv.172348951.12175366/v1 — e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...\n",
      "Optimizing LLM Inference Clusters for Enhanced Performance and\n",
      "Energy Eﬃciency\n",
      "Soka Hisaharo1, Yuki Nishimura1, and Aoi Takahashi1\n",
      "1Aﬃliation not available\n",
      "August 12, 2024\n",
      "Abstract\n",
      "The growing demand for eﬃcient and scalable AI solutions has driven research into optimizing the performance and energy\n",
      "\n",
      "\n",
      "\n",
      "--- paper4 ---\n",
      "Applied Energy 367 (2024) 123431\n",
      "Available online 16 May 2024\n",
      "0306-2619/© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n",
      "EPlus-LLM: A large language model-based computing platform for \n",
      "automated building energy modeling \n",
      "Gang Jiang a, Zhihao Ma a, Liang Zhang b, Jianli Chen a,* \n",
      "a The University of Utah, United States \n",
      "b The University of Arizona, United States   \n",
      "H I G H L I G H T S  \n",
      "• This study represents the pione\n",
      "\n",
      "\n",
      "--- paper5 ---\n",
      "DynamoLLM: Designing LLM Inference Clusters\n",
      "for Performance and Energy Efficiency\n",
      "Jovan Stojkovic, Chaojie Zhang†, ´I˜nigo Goiri†, Josep Torrellas, Esha Choukse†\n",
      "University of Illinois at Urbana-Champaign\n",
      "†Microsoft Azure Research - Systems\n",
      "Abstract—The rapid evolution and widespread adoption of\n",
      "generative large language models (LLMs) have made them a\n",
      "pivotal workload in various applications. Today, LLM inference\n",
      "clusters receive a large number of queries with strict Service\n",
      "Level Objectives (SL\n",
      "\n",
      "\n",
      "--- paper6 ---\n",
      "Large Language Model (LLM)-enabled In-context\n",
      "Learning for Wireless Network Optimization: A\n",
      "Case Study of Power Control\n",
      "Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu,\n",
      "Xue Liu, Fellow, IEEE, and Charlie Zhang, Fellow, IEEE.\n",
      "Abstract—Large language model (LLM) has recently been\n",
      "considered a promising technique for many fields. This work\n",
      "explores LLM-based wireless network optimization via in-context\n",
      "learning. To showcase the potential of LLM technologies, we\n",
      "consider the base station (BS) power\n",
      "\n",
      "\n",
      "--- paper7 ---\n",
      "Hybrid Heterogeneous Clusters Can Lower the Energy\n",
      "Consumption of LLM Inference Workloads\n",
      "Grant Wilkins\n",
      "gfw27@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Srinivasan Keshav\n",
      "sk818@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Richard Mortier\n",
      "rmm1002@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "ABSTRACT\n",
      "Both the training and use of Large Language Models (LLMs) require\n",
      "large amounts of energy. Their increasing popularity, therefore,\n",
      "raises critical concerns regarding the energy efficiency a\n",
      "\n",
      "\n",
      "--- paper8 ---\n",
      "Can Private LLM Agents Synthesize Household Energy\n",
      "Consumption Data?\n",
      "Mahathir Almashor∗\n",
      "mahathir.almashor@csiro.au\n",
      "CSIRO Energy\n",
      "Sydney, Australia\n",
      "Yusuke Miyashita∗\n",
      "yusuke.miyashita@csiro.au\n",
      "CSIRO Energy\n",
      "Melbourne, Australia\n",
      "Sam West∗\n",
      "sam.west@csiro.au\n",
      "CSIRO Energy\n",
      "Newcastle, Australia\n",
      "Thi Van Dai Dong∗\n",
      "thivandai.dong@csiro.au\n",
      "CSIRO Energy\n",
      "Wollonggong, Australia\n",
      "ABSTRACT\n",
      "Reproducible science requires easy access to data, especially with\n",
      "the rise of data-driven and increasingly complex models used\n",
      "\n",
      "\n",
      "--- paper9 ---\n",
      "Democratizing Energy Management with LLM-Assisted Optimization\n",
      "Autoformalism\n",
      "Ming Jin, Bilgehan Sel, Fnu Hardeep, Wotao Yin\n",
      "Abstract— This paper introduces a method for personaliz-\n",
      "ing energy optimization using large language models (LLMs)\n",
      "combined with an optimization solver. This approach, termed\n",
      "human-guided optimization autoformalism, translates natural\n",
      "language speciﬁcations into optimization problems, enabling\n",
      "LLMs to handle various user-speciﬁc energy-related tasks. It\n",
      "allows for nuanced \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return read_pdf(fitz.open(pdf_path))\n",
    "    \n",
    "# Extract text from all uploaded PDF files\n",
    "pdf_texts = {f'paper{i}': extract_text_from_pdf(f'paper{i}.pdf') for i in range(10)}\n",
    "\n",
    "#Display the text from all the PDF files\n",
    "for pdf_file, text in pdf_texts.items(): \n",
    "    print(f\"--- {pdf_file} ---\")\n",
    "    print(text[:500])  # Display the first 500 characters of each document\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an index of vectors to represent the documents\n",
    "\n",
    "To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n",
    "\n",
    "Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3220, -0.3335, -0.1954,  ..., -0.2925,  0.2846,  0.3641],\n",
      "         [-0.5645,  0.4611,  0.7577,  ..., -0.2724,  0.3017, -0.3110],\n",
      "         [-0.9413, -0.0867,  1.0387,  ..., -0.2631,  0.0400,  0.5614],\n",
      "         ...,\n",
      "         [-0.1484, -0.2075, -0.1752,  ...,  0.0926, -0.1863,  0.1050],\n",
      "         [-0.2090, -0.1377, -0.0613,  ..., -0.0501, -0.2226,  0.1268],\n",
      "         [-0.0029, -0.0475, -0.0555,  ..., -0.1070, -0.2341,  0.2976]],\n",
      "\n",
      "        [[-0.5559, -0.2410, -0.5079,  ..., -0.3529,  0.4078, -0.1399],\n",
      "         [ 0.2008,  0.7545, -0.3053,  ...,  0.0627,  0.5156, -0.0106],\n",
      "         [ 0.0417,  0.2561,  0.0759,  ..., -0.6539,  0.1648,  0.0106],\n",
      "         ...,\n",
      "         [-0.3017, -0.4531,  0.0994,  ...,  0.0924,  0.0874, -0.8269],\n",
      "         [ 0.5455,  0.6252,  0.0647,  ..., -0.2675, -0.0738, -0.1672],\n",
      "         [ 0.2909,  0.0319,  0.4529,  ..., -0.0058, -0.0636, -0.6091]],\n",
      "\n",
      "        [[-0.4784, -0.1242, -0.2999,  ..., -0.3500,  0.0410,  0.1079],\n",
      "         [-0.3934,  0.0633, -0.0464,  ...,  0.0548,  0.9342, -0.1956],\n",
      "         [-1.0155, -0.0706,  0.6362,  ..., -0.4300,  0.6091, -0.2326],\n",
      "         ...,\n",
      "         [-0.1993, -0.0719,  0.1351,  ..., -0.4016,  0.0854,  0.1119],\n",
      "         [-0.0513, -0.0283,  0.2427,  ..., -0.2995, -0.1180,  0.1526],\n",
      "         [-0.0594, -0.0676,  0.0629,  ..., -0.2579,  0.1957,  0.1691]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5542, -0.3113, -0.1209,  ..., -0.4350,  0.1075,  0.3344],\n",
      "         [-0.5100,  0.5568, -0.3331,  ...,  0.2765,  1.2254, -0.0555],\n",
      "         [-0.5473,  0.4522, -0.0159,  ..., -0.0873,  0.0038, -0.2086],\n",
      "         ...,\n",
      "         [ 0.2935, -0.2061,  0.2980,  ..., -0.3086,  0.1146, -0.0121],\n",
      "         [ 0.3889, -0.4874,  0.2581,  ..., -0.1634, -0.1608,  0.1081],\n",
      "         [ 0.4208, -0.3350,  0.3135,  ..., -0.3281,  0.1220,  0.1022]],\n",
      "\n",
      "        [[-0.4625, -0.1280, -0.2624,  ..., -0.1981,  0.2320, -0.0254],\n",
      "         [-0.1633, -0.1647, -0.0728,  ...,  0.1052,  0.7913,  0.3412],\n",
      "         [-0.4391, -0.1115,  1.1007,  ..., -0.0373, -0.2194, -0.4902],\n",
      "         ...,\n",
      "         [ 0.4658, -0.0659,  0.3852,  ..., -0.1840,  0.0738,  0.0255],\n",
      "         [ 0.7635,  0.1433,  0.1409,  ..., -0.5854,  0.2161, -0.2955],\n",
      "         [-0.2771, -0.2029,  0.2326,  ...,  0.0961,  0.0585, -0.1100]],\n",
      "\n",
      "        [[-0.4782, -0.3128, -0.5707,  ..., -0.0917,  0.0700,  0.1525],\n",
      "         [-0.4275, -0.1753, -0.5658,  ...,  0.3142,  1.0482,  0.1641],\n",
      "         [-0.5393, -0.0117, -0.8624,  ...,  0.4734, -0.2629, -0.1614],\n",
      "         ...,\n",
      "         [-0.0273, -0.0771,  0.4547,  ..., -0.0180,  0.0108, -0.0233],\n",
      "         [ 0.0799, -0.0287,  0.5449,  ..., -0.1848,  0.0124,  0.0354],\n",
      "         [ 0.0326, -0.0729,  0.4946,  ..., -0.0635, -0.0462, -0.0232]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.5546, -0.4844, -0.9895,  ..., -0.9440, -0.5674,  0.1592],\n",
      "        [-0.5724, -0.5698, -0.9860,  ..., -0.9805, -0.5232, -0.0460],\n",
      "        [-0.6487, -0.4944, -0.9599,  ..., -0.9442, -0.6129,  0.0594],\n",
      "        ...,\n",
      "        [-0.3579, -0.3729, -0.9367,  ..., -0.9562, -0.4630, -0.1761],\n",
      "        [-0.5625, -0.4695, -0.9734,  ..., -0.9610, -0.4394, -0.0790],\n",
      "        [-0.7205, -0.4696, -0.9680,  ..., -0.9513, -0.5913,  0.1746]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import the Bert pretrained model from the transformers library\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(device)\n",
    "\n",
    "#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n",
    "#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n",
    "abstracts_dict = {\n",
    "    0: \"\"\"Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-\n",
    "of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific\n",
    "field, it’s challenging to acquire unique domain knowledge while keeping the model itself advanced.\n",
    "To address this challenge, a sophisticated large language model system named as Xiwu has been\n",
    "developed, allowing you switch between the most advanced foundation models and quickly teach the\n",
    "model domain knowledge. In this work, we will report on the best practices for applying LLMs in the\n",
    "field of high-energy physics (HEP), including: a seed fission technology is proposed and some data\n",
    "collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time\n",
    "learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system\n",
    "has been developed to facilitate rapid training under a specified foundation model.\n",
    "The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna,\n",
    "ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model\n",
    "on the HEP knowledge Q&A and code generation. This strategy significantly enhances the potential\n",
    "for growth of our model’s performance, with the hope of surpassing GPT-4 as it evolves with the\n",
    "development of open-source models. This work provides a customized LLM for the field of HEP,\n",
    "while also offering references for applying LLM to other fields, the corresponding codes are available\n",
    "on Github https://github.comzhang/zhengde0225/Xiwu.\"\"\",\n",
    "    1: \"\"\"\n",
    "Abstract—With the ubiquitous use of modern large language\n",
    "models (LLMs) across industries, the inference serving for these\n",
    "models is ever expanding. Given the high compute and memory\n",
    "requirements of modern LLMs, more and more top-of-the-\n",
    "line GPUs are being deployed to serve these models. Energy\n",
    "availability has come to the forefront as the biggest challenge for\n",
    "data center expansion to serve these models. In this paper, we\n",
    "present the trade-offs brought up by making energy efficiency\n",
    "the primary goal of LLM serving under performance SLOs.\n",
    "We show that depending on the inputs, the model, and the\n",
    "service-level agreements, there are several knobs available to\n",
    "the LLM inference provider to use for being energy efficient.\n",
    "We characterize the impact of these knobs on the latency,\n",
    "throughput, as well as the energy. By exploring these trade-\n",
    "offs, we offer valuable insights into optimizing energy usage\n",
    "without compromising on performance, thereby paving the way\n",
    "for sustainable and cost-effective LLM deployment in data center\n",
    "environments\n",
    "    \"\"\",\n",
    "    2: \"\"\"\n",
    "The rapid adoption of large language models (LLMs) has led to\n",
    "significant advances in natural language processing and text gener-\n",
    "ation. However, the energy consumed through LLM model infer-\n",
    "ence remains a major challenge for sustainable AI deployment. To\n",
    "address this problem, we model the workload-dependent energy\n",
    "consumption and runtime of LLM inference tasks on heterogeneous\n",
    "GPU-CPU systems. By conducting an extensive characterization\n",
    "study of several state-of-the-art LLMs and analyzing their energy\n",
    "and runtime behavior across different magnitudes of input prompts\n",
    "and output text, we develop accurate (𝑅2 >0.96)energy and run-\n",
    "time models for each LLM. We employ these models to explore\n",
    "an offline, energy-optimal LLM workload scheduling framework.\n",
    "Through a case study, we demonstrate the advantages of energy\n",
    "and accuracy aware scheduling compared to existing best practices\n",
    "    \"\"\",\n",
    "    3: \"\"\"\n",
    "The growing demand for efficient and scalable AI solutions has driven research into optimizing the performance and energy\n",
    "efficiency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo\n",
    "model offers a significant advancement in addressing the computational and environmental challenges associated with AI\n",
    "deployment. By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes,\n",
    "the research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced\n",
    "interconnect technologies, high-bandwidth memory modules, and energy-efficient power management techniques, alongside\n",
    "software optimizations, enabled the redesigned clusters to outperform baseline models significantly. Empirical evaluations\n",
    "demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable\n",
    "AI technologies. The findings underscore the importance of balancing performance with energy efficiency and provide a robust\n",
    "framework for future research and development in AI optimization. The research contributes valuable insights into the design\n",
    "and deployment of more efficient and environmentally responsible AI systems.\n",
    "1\n",
    "\"\"\",\n",
    "    4: \"\"\"\n",
    "Establishing building energy models (BEMs) for building design and analysis poses significant challenges due to demanding modeling efforts, expertise to use simulation software, and building science knowledge in practice. These make building modeling labor-intensive, hindering its widespread adoptions in building development. Therefore, to overcome these challenges in building modeling with enhanced automation in modeling practice, this paper proposes Eplus-LLM (EnergyPlus-Large Language Model) as the auto-building modeling platform, building on a fine-tuned large language model (LLM) to directly translate natural language description of buildings to established building models of various geometries, occupancy scenarios, and equipment loads. Through fine-tuning, the LLM (i.e., T5) is customized to digest natural language and simulation demands from users and convert human descriptions into EnergyPlus modeling files. Then, the Eplus-LLM platform realizes the automated building modeling through invoking the API of simulation software (i.e., the EnergyPlus engine) to simulate the auto-generated model files and output simulation results of interest. The validation process, involving four different types of prompts, demonstrates that Eplus-LLM reduces over 95% modeling efforts and achieves 100% accuracy in establishing BEMs while being robust to interference in usage, including but not limited to different tones, misspells, omissions, and redundancies. Overall, this research serves as the pioneering effort to customize LLM for auto-modeling purpose (directly build-up building models from natural language), aiming to provide a user-friendly human-AI interface that significantly reduces building modeling efforts. This work also further facilitates large-scale building model efforts, e.g., urban building energy modeling (UBEM), in modeling practice.\n",
    "\"\"\",\n",
    "5: \"\"\"\n",
    "The rapid evolution and widespread adoption of\n",
    "generative large language models (LLMs) have made them a\n",
    "pivotal workload in various applications. Today, LLM inference\n",
    "clusters receive a large number of queries with strict Service\n",
    "Level Objectives (SLOs). To achieve the desired performance,\n",
    "these models execute on power-hungry GPUs causing the in-\n",
    "ference clusters to consume large amount of energy and, conse-\n",
    "quently, result in excessive carbon emissions. Fortunately, we find\n",
    "that there is a great opportunity to exploit the heterogeneity in\n",
    "inference compute properties and fluctuations in inference work-\n",
    "loads, to significantly improve energy-efficiency. However, such\n",
    "a diverse and dynamic environment creates a large search-space\n",
    "where different system configurations (e.g., number of instances,\n",
    "model parallelism, and GPU frequency) translate into different\n",
    "energy-performance trade-offs. To address these challenges, we\n",
    "propose DynamoLLM, the first energy-management framework\n",
    "for LLM inference environments. DynamoLLM automatically\n",
    "and dynamically reconfigures the inference cluster to optimize for\n",
    "energy and cost of LLM serving under the service’s performance\n",
    "SLOs. We show that at a service-level, DynamoLLM conserves\n",
    "53 percent energy and 38 percent operational carbon emissions, and reduces\n",
    "61 percent cost to the customer, while meeting the latency SLOs\n",
    "\"\"\",\n",
    "6: \"\"\"\n",
    "Large language model (LLM) has recently been\n",
    "considered a promising technique for many fields. This work\n",
    "explores LLM-based wireless network optimization via in-context\n",
    "learning. To showcase the potential of LLM technologies, we\n",
    "consider the base station (BS) power control as a case study,\n",
    "a fundamental but crucial technique that is widely investigated\n",
    "in wireless networks. Different from existing machine learning\n",
    "(ML) methods, our proposed in-context learning algorithm relies\n",
    "on LLM’s inference capabilities. It avoids the complexity of\n",
    "tedious model training and hyper-parameter fine-tuning, which is\n",
    "a well-known bottleneck of many ML algorithms. Specifically, the\n",
    "proposed algorithm first describes the target task via formatted\n",
    "natural language, and then designs the in-context learning\n",
    "framework and demonstration examples. After that, it considers\n",
    "two cases, namely discrete-state and continuous-state problems,\n",
    "and proposes state-based and ranking-based methods to select\n",
    "appropriate examples for these two cases, respectively. Finally, the\n",
    "simulations demonstrate that the proposed algorithm can achieve\n",
    "comparable performance as conventional deep reinforcement\n",
    "learning (DRL) techniques without dedicated model training or\n",
    "fine-tuning. Such an efficient and low-complexity approach has\n",
    "great potential for future wireless network optimization.\n",
    "Index Terms—Large language model, in-context learning, net-\n",
    "work optimization, transmission power contro\n",
    "\"\"\",\n",
    "7: \"\"\"\n",
    "Both the training and use of Large Language Models (LLMs) require\n",
    "large amounts of energy. Their increasing popularity, therefore,\n",
    "raises critical concerns regarding the energy efficiency and sus-\n",
    "tainability of data centers that host them. This paper addresses the\n",
    "challenge of reducing energy consumption in data centers running\n",
    "LLMs. We propose a hybrid data center model that uses a cost-based\n",
    "scheduling framework to dynamically allocate LLM tasks across\n",
    "hardware accelerators that differ in their energy efficiencies and\n",
    "computational capabilities. Specifically, our workload-aware strat-\n",
    "egy determines whether tasks are processed on energy-efficient\n",
    "processors or high-performance GPUs based on the number of in-\n",
    "put and output tokens in a query. Our analysis of a representative\n",
    "LLM dataset, finds that this hybrid strategy can reduce CPU+GPU\n",
    "energy consumption by 7.5 percent compared to a workload-unaware\n",
    "baseline\n",
    "\"\"\",\n",
    "8: \"\"\"\n",
    "Reproducible science requires easy access to data, especially with\n",
    "the rise of data-driven and increasingly complex models used within\n",
    "energy research. Too often however, the data to reconstruct and\n",
    "verify purported solutions in publications is hidden due to some\n",
    "combination of commercial, legal, and sensitivity issues. This early\n",
    "work presents our initial efforts to leverage the recent advance-\n",
    "ments in Large Language Models (LLMs) to create usable and share-\n",
    "able energy datasets. In particular, we’re utilising their mimicry of\n",
    "human behaviors, with the goal of extracting and exploring syn-\n",
    "thetic energy data through the simulation of LLM agents capable of\n",
    "interacting with and executing actions in controlled environments.\n",
    "We also analyse and visualise publicly available data in an attempt\n",
    "to create realistic but not quite exact copies of the originals. Our\n",
    "early results show some promise, with outputs that resemble the\n",
    "twin peak curves for household energy consumption. The hope is\n",
    "that our generalised approach can be used to easily replicate usable\n",
    "and realistic copies of otherwise secret or sensitive data\n",
    "\"\"\",\n",
    "9: \"\"\"\n",
    "This paper introduces a method for personaliz-\n",
    "ing energy optimization using large language models (LLMs)\n",
    "combined with an optimization solver. This approach, termed\n",
    "human-guided optimization autoformalism, translates natural\n",
    "language specifications into optimization problems, enabling\n",
    "LLMs to handle various user-specific energy-related tasks. It\n",
    "allows for nuanced understanding and nonlinear reasoning\n",
    "tailored to individual preferences. The research covers common\n",
    "energy sector tasks like electric vehicle charging, HVAC control,\n",
    "and long-term planning for renewable energy installations. This\n",
    "novel strategy represents a significant advancement in context-\n",
    "based optimization using LLMs, facilitating sustainable energy\n",
    "practices customized to individual needs\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "#the text for rag is used as an input to the BERT model\n",
    "\n",
    "#The tokenized inputs are passed to the BERT model for processing.\n",
    "#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n",
    "#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n",
    "#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n",
    "#batch_size: Number of input texts.\n",
    "#sequence_length: Length of each tokenized text (after padding).\n",
    "#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n",
    "\n",
    "#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n",
    "tokenized_inputs = tokenizer(list(abstracts_dict.values()), return_tensors='pt', padding=True)\n",
    "#same but with values on GPU\n",
    "tokenized_inputs = {key: value.to(device) for key, value in tokenized_inputs.items()}\n",
    "\n",
    "print(model(**tokenized_inputs))\n",
    "abstract_vectors = model(**tokenized_inputs).last_hidden_state[:, 0]\n",
    "\n",
    "#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector (CLS).\n",
    "\n",
    "print(abstract_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n",
    "\n",
    "To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n",
    "\n",
    "To compute cosine similarity, refer to the following formula\n",
    "\n",
    "```cs = cosine_similarity(vector_a.cpu().detach().numpy(), vector_b.cpu().detach().numpy())```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n",
      "Query tensor is on device: cuda:0\n",
      "BEST DOCUMENTS: [[9]]\n",
      "Democratizing Energy Management with LLM-Assisted Optimization\n",
      "Autoformalism\n",
      "Ming Jin, Bilgehan Sel, Fnu Hardeep, Wotao Yin\n",
      "Abstract— This paper introduces a method for personaliz-\n",
      "ing energy optimization using large language models (LLMs)\n",
      "combined with an optimization solver. This approach, termed\n",
      "human-guided optimization autoformalism, translates natural\n",
      "language speciﬁcations into optimization problems, enabling\n",
      "LLMs to handle various user-speciﬁc energy-related tasks. It\n",
      "allows for nuanced understanding and nonlinear reasoning\n",
      "tailored to individual preferences. The research covers common\n",
      "energy sector tasks like electric vehicle charging, HVAC control,\n",
      "and long-term planning for renewable energy installations. This\n",
      "novel strategy represents a signiﬁcant advancement in context-\n",
      "based optimization using LLMs, facilitating sustainable energy\n",
      "practices customized to individual needs.\n",
      "I. INTRODUCTION\n",
      "Despite computational advances, the complex challenges\n",
      "of meeting energy demands and reducing carbon emissions\n",
      "persist. Optimization techniques, as seen in EV charging [1],\n",
      "energy storage [2], renewable investments [3], smart building\n",
      "operations [4], and demand side management [5], hold great\n",
      "promise. However, the broader democratization of these\n",
      "tools remains a signiﬁcant hurdle. This paper tackles this\n",
      "issue, advocating for the accessibility and practicality of\n",
      "advanced computational methods for all, especially the un-\n",
      "derserved [6]. The emergence of LLMs offers a breakthrough\n",
      "in overcoming these barriers. We demonstrate that LLMs can\n",
      "bridge the gap between the high costs of traditional optimiza-\n",
      "tion and the need for personalized, accessible solutions. Our\n",
      "goal is not industry-grade optimization, but rather to provide\n",
      "users with tools for informed decision-making, potentially\n",
      "involving optimization problem formulation. This approach\n",
      "leverages LLMs to streamline modeling and optimization,\n",
      "enabling interaction through natural language without exten-\n",
      "sive programming or mathematical optimization knowledge.\n",
      "Technical challenges and solutions. While LLMs such as\n",
      "OpenAI’s ChatGPT offer a natural substrate for conversa-\n",
      "tional AI (a technology that enables machines to understand,\n",
      "interpret, and engage in natural conversations), it is essential\n",
      "to recognize that current LLMs have not excelled in tasks like\n",
      "arithmetic and logical reasoning [7]. Recent works have in-\n",
      "troduced methods such as chain of thoughts [8] (and variants\n",
      "such as algorithm of thoughts [9]), which prompts a series\n",
      "of intermediate reasoning steps, and autoformalism [10],\n",
      "which automatically translates natural language mathematics\n",
      "to formal speciﬁcations and proofs. However, many energy\n",
      "problems, like energy storage control and long-term planning\n",
      "for PV panel installation, demand complex decision-making.\n",
      "These problems differ from arithmetic reasoning, common-\n",
      "sense reasoning, and symbolic reasoning in the following\n",
      "aspects: Complexity, as they often involve numerous vari-\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Solar panel\n",
      "HVAC control\n",
      "Heat pump\n",
      "EV charging\n",
      "Battery\n",
      "charging\n",
      "Battery\n",
      "capacity\n",
      "Success rate (%)\n",
      "Problem category\n",
      "Baseline\n",
      "EC (CVXPY)\n",
      "EC (SCIPY)\n",
      "Fig. 1: Comparison of the baseline method (simple prompt-\n",
      "ing with GPT-4) and EC using either CVXPY or SCIPY as\n",
      "the code projection layer across various problem categories.\n",
      "Success rate denotes the percentage of instances in which the\n",
      "provided answer aligns with user expectations, satisﬁes all\n",
      "given constraints, and is either close to or exactly optimal.\n",
      "ables, constraints, and objectives with potentially nonlinear\n",
      "relationships between variables; and incomplete information,\n",
      "as energy systems are inﬂuenced by various factors, such as\n",
      "user preferences, which may not be provided in the initial\n",
      "problem description.\n",
      "A. Contributions\n",
      "In this research, we address the above challenges as\n",
      "follows. To manage complexity, we develop a procedure\n",
      "to map a problem description to code using the grammar\n",
      "of packages that support optimization formulation, and to\n",
      "iteratively formulate, debug, and execute the program. We\n",
      "also introduce external tooling to execute the written code\n",
      "and solve the formulated optimization using dedicated al-\n",
      "gorithms. Furthermore, we implement prompt engineering\n",
      "techniques to enable LLMs to accurately understand and\n",
      "respond to user-speciﬁc preferences. To address incomplete\n",
      "information, we leverage the reasoning capabilities of LLMs\n",
      "to identify key parameters and use a question-and-answer for-\n",
      "mat in natural conversations to solicit this information from\n",
      "users. Additionally, we employ LLMs for auto-informalism\n",
      "to explain the solution to users. An overview of the EC\n",
      "framework is depicted in Fig. 2. This approach bears dual\n",
      "beneﬁts: it not only guides the user through complex nonlin-\n",
      "ear reasoning tasks of energy saving planning but also offers\n",
      "an insightful explanation of the solution.\n",
      "We further demonstrate the possibility of our proposed ap-\n",
      "proach in solving a variety of tasks within the energy domain,\n",
      "ranging from EV charging, HVAC and battery control, to\n",
      "long-term planning problems such as cost-beneﬁt evaluation\n",
      "of installing rooftop solar PVs, heat pumps, and battery\n",
      "sizing. In comparison to the simple prompting method,\n",
      "where the task description is directly presented to the LLM,\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
      "979-8-3503-1855-5/24/$31.00 ©2024 IEEE\n",
      "258\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm) | 979-8-3503-1855-5/24/$31.00 ©2024 IEEE | DOI: 10.1109/SmartGridComm60555.2024.10738100\n",
      "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
      "our proposed EC framework improves the success rate, as\n",
      "illustrated in Fig. 1. Although the optimizations formulated\n",
      "are not exceedingly sophisticated, we ﬁnd that the LLM\n",
      "effectively addresses the problem to a considerable degree.\n",
      "Throughout the remainder of the paper, unless speciﬁed\n",
      "otherwise, we refer to OpenAI’s ChatGPT (GPT-4) when\n",
      "mentioning LLM.\n",
      "B. Related Work\n",
      "Recent work has explored using LLMs for mathematical\n",
      "autoformalization, converting natural language into formal\n",
      "languages like Isabelle/HOL [11]. Analogously, we use in-\n",
      "termediaries like SCIPY and CVXPY to transform intuitive\n",
      "formulations of optimization problems into standard form\n",
      "[12]. Despite few CVXPY examples online, LLMs exhibit\n",
      "surprising adeptness, albeit with occasional syntax errors that\n",
      "we correct via Python debugging. While LLMs struggle with\n",
      "multi-step problems [13], techniques like in-context learning\n",
      "[14] and algorithmic search [9] have shown promise. We\n",
      "present a novel viewpoint using optimization itself as a rea-\n",
      "soning tool to enrich existing LLM augmentation techniques\n",
      "[15]. LLMs can also revise responses given feedback, e.g. in\n",
      "question-answering and code debugging [16]. We incorporate\n",
      "revisions based on programming language feedback and user\n",
      "requests, enabling rapid error correction.\n",
      "Optimizing energy systems requires commonsense reason-\n",
      "ing and domain knowledge [17]. Prior work has used ML\n",
      "and expert systems [18], but we harness LLM expertise for\n",
      "optimization through natural language interaction. To our\n",
      "knowledge, we are the ﬁrst to tackle non-trivial, energy-\n",
      "speciﬁc problems with incomplete information this way.\n",
      "The remaining sections of this paper are structured as\n",
      "follows. Section II describes our proposed framework in\n",
      "detail, including the design principles and the optimization\n",
      "autoformalism approach. Section III presents experimental\n",
      "results that demonstrate the effectiveness of our approach\n",
      "in solving various energy-related tasks. We discuss the po-\n",
      "tential and future directions of conversational AI for energy\n",
      "sustainability and conclude in Section IV.\n",
      "II. ENERGY CONCIERGE FRAMEWORK\n",
      "Using an EV charging query as an example, our Energy\n",
      "Concierge operates as follows:\n",
      "1) User submits natural language request (e.g. optimize\n",
      "my charging schedule). LLM determines if it is an\n",
      "optimization problem.\n",
      "2) If so, LLM requests input parameters needed to solve\n",
      "it. It may ask clarifying questions (e.g. charging ca-\n",
      "pacity? preferred hours?).\n",
      "3) LLM formulates Python code with user information,\n",
      "translating the query into an optimization problem via\n",
      "autoformalism.\n",
      "4) Interface executes code to solve problem, with debug-\n",
      "ging iterations if needed.\n",
      "5) LLM explains the optimal solution clearly (charging\n",
      "schedule, cost savings). Enabling informed user deci-\n",
      "sions.\n",
      "Fig. 2: Energy Concierge framework. The user engages with\n",
      "an LLM through natural language queries and responses.\n",
      "The LLM identiﬁes the necessary input parameters for opti-\n",
      "mization and generates Python code to address the problem.\n",
      "The program interface then executes the code and relays the\n",
      "solution back to the LLM, which subsequently provides a\n",
      "clear explanation to the user.\n",
      "Through interactivity and personalization grounded in opti-\n",
      "mization and autoformalism, this conversational framework\n",
      "empowers users to improve energy efﬁciency.\n",
      "A. Optimization autoformalism\n",
      "A general optimization problem can be written as:\n",
      "minimizex2Rp\n",
      "f(x; ✓)\n",
      "subject to\n",
      "gi(x; ✓) 0,\n",
      "i = 1, 2, . . . , m\n",
      "hj(x; ✓) = 0,\n",
      "j = 1, 2, . . . , n,\n",
      "(P(✓))\n",
      "where we seek to minimize the objective function f(x; ✓)\n",
      "subject to a set of inequality constraints gi(x; ✓) 0, and\n",
      "equality constraints hj(x; ✓) = 0. Here, x 2 Rp is the\n",
      "decision variable and ✓is the collection of hyperparameters\n",
      "that deﬁnes the optimization instance, including objective\n",
      "and constraint functions; in other words, the solution of\n",
      "the optimization P(✓) can be regarded as a function of the\n",
      "hyperparameters ✓[19].\n",
      "Optimization techniques can be used to solve energy-\n",
      "related problems [1]–[5]. Automating the formulation and\n",
      "solution of optimization problems is essential due to the tech-\n",
      "nical skills gap, challenges in manually incorporating user\n",
      "preferences and constraints, and the inefﬁciencies in manual\n",
      "modiﬁcations based on user feedback. The human-guided\n",
      "autoformalism proposed in this study automatically translates\n",
      "natural language task speciﬁcations to optimization instances.\n",
      "However, directlying implementing this approach may face\n",
      "issues such as ambiguity, incompleteness, and incorporating\n",
      "user-speciﬁc preferences. The subsequent subsections will\n",
      "delineate strategies to address them.\n",
      "1) Optimization formulation: The optimization process\n",
      "begins with identifying the objective function, decision vari-\n",
      "ables, and constraints from the user’s task description [T].\n",
      "We tested two approaches:\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
      "259\n",
      "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
      "• Approach 1: Directly ask the LLM to identify the\n",
      "optimization components in [T].\n",
      "• Approach 2: First prompt the LLM to identify the\n",
      "5 most important parameters in [T], then use these\n",
      "parameters to formulate the optimization instance.\n",
      "Approach 2 outperforms Approach 1 because LLMs like\n",
      "GPT-4 are trained on diverse data, not just optimization tasks.\n",
      "Asking for the top 5 parameters simpliﬁes the task and pro-\n",
      "vides context, helping LLMs generate more precise optimiza-\n",
      "tion formulations [8], [9]. Directly requesting optimization\n",
      "formulations (Approach 1) can introduce ambiguityDirectly\n",
      "requesting optimization formulations can introduce ambigu-\n",
      "ity; initial parameter identiﬁcation (Approach 2) reduces this\n",
      "uncertainty.\n",
      "After identifying the essential parameters, the natural\n",
      "language query is transformed into a computational instance\n",
      "using the prompt: “Write a Python code using [lib] to solve\n",
      "this optimization problem,” where [lib] is either CVXPY or\n",
      "SCIPY. We found that SCIPY yields a higher success rate\n",
      "(71% to 100%) compared to CVXPY (51% to 80%) (see\n",
      "Fig. 1). This difference is likely due to SCIPY’s broader\n",
      "acceptance and longer existence in the Python community,\n",
      "resulting in more SCIPY-related content available online for\n",
      "the model to leverage during training.1\n",
      "2) Solving an optimization and debugging: To address\n",
      "LLMs’ limitations in nonlinear reasoning [7], we enable\n",
      "the model to interact with an external Python program to\n",
      "solve optimization tasks. We extract the code block from\n",
      "the LLM’s output using regular expressions, searching for\n",
      "unique delimiters enclosing the code. This method reliably\n",
      "extracts code without requiring a large labeled dataset, unlike\n",
      "training a machine learning model [20]. During development,\n",
      "we encountered two types of errors in the LLM-generated\n",
      "code: erroneous translation into an optimization problem and\n",
      "syntactic bugs in an otherwise correctly translated problem.2\n",
      "To rectify translation errors, we rely on user interaction\n",
      "and clariﬁcation of the formulated optimization. For syntactic\n",
      "errors, we use an automated process: identify the error\n",
      "message, feed it to the LLM to isolate relevant code snippets,\n",
      "generate potential remedies, and assess the proposed solu-\n",
      "tions in a new iteration (Fig. 7 in Appendix). Taking multiple\n",
      "code samples before debugging often results in fewer LLM\n",
      "queries (Fig. 5). By following this procedure, we successfully\n",
      "resolved most errors encountered during our experiments.\n",
      "3) Optimization auto-informalism: After discovering the\n",
      "optimal solution, the LLM articulates the results in a com-\n",
      "prehensible, natural language format. This system provides\n",
      "detailed explanations of the optimal solution and any con-\n",
      "straints or preferences factored in during the optimization\n",
      "process (Steps 7a and 7b in Fig. 2). This auto-informalism\n",
      "approach complements autoformalism by offering intuitive\n",
      "1A search on GitHub (as of 4/25/24) returns 8K repositories for ”SCIPY”\n",
      "and only 272 for ”CVXPY,” supporting our preference for SCIPY.\n",
      "2Errors can also arise due to the limitations of the optimization solvers,\n",
      "but since most energy problems involve linear or quadratic convex opti-\n",
      "mization, we presume such errors are comparatively infrequent. We do not\n",
      "classify infeasibility as an error.\n",
      "0\n",
      "0.03\n",
      "0.06\n",
      "0.09\n",
      "0.12\n",
      "0.15\n",
      "0.18\n",
      "0.21\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "6pm\n",
      "7pm\n",
      "8pm\n",
      "9pm\n",
      "10pm\n",
      "11pm\n",
      "12am\n",
      "1am\n",
      "2am\n",
      "3am\n",
      "4am\n",
      "5am\n",
      "Electricity Price ($)\n",
      "Charging Power (kW)\n",
      "Common Charging\n",
      "Baseline\n",
      "EC\n",
      "Price\n",
      "Fig. 3: A comparison of EV charging plans to improve\n",
      "the cost and load on the grid. Observations reveal that EC\n",
      "effectively capitalizes on hours with lower prices in contrast\n",
      "to the baseline.\n",
      "insight into the optimization outcomes.\n",
      "III. EXPERIMENTS AND CASE STUDIES\n",
      "This section examines the potential of LLM-based auto-\n",
      "formalism in real-time decision-making (Sec. III-A) and sus-\n",
      "tainable long-term planning (Sec. III-B). A general analysis\n",
      "of the proposed methods is provided in Sec. III-C. The [21,\n",
      "appendix] offers example interactions.\n",
      "A. Real-time decision making\n",
      "1) Smart EV charging: The core issue in smart EV\n",
      "charging is to optimize EV charging patterns to balance\n",
      "power grid loads, mitigate energy costs, and fulﬁll user pref-\n",
      "erences [1]. EC can correspond user-provided information to\n",
      "optimization parameters, such as the maximum charging rate.\n",
      "It grasps the user’s charging availability and translates this\n",
      "information into decision variables and constraints, leading\n",
      "to an optimized schedule (see Fig. 3).\n",
      "2) HVAC control: HVAC control is a pivotal issue that\n",
      "has been extensively studied [4]. However, the algorithms\n",
      "that have been developed are often more advanced than\n",
      "the current control panels in most buildings, leading to a\n",
      "disconnect between sophisticated methods and user compre-\n",
      "hension. The heart of the issue is to discover a setpoint\n",
      "that ensures a comfortable indoor climate while minimizing\n",
      "energy consumption. EC provides clear and actionable advice\n",
      "like “Set your thermostat to the optimal temperature (75◦F in\n",
      "this case) during hot, humid days...” with a clear rationale. It\n",
      "also offers customized suggestions based on user conditions,\n",
      "along with valuable energy efﬁciency tips such as “monitor\n",
      "your energy consumption”, and “adapt to changing condi-\n",
      "tions”. Nevertheless, the cost of simplicity is the inability\n",
      "to perform sophisticated controls like pre-heating/cooling,\n",
      "which entails heating the room in anticipation of occupancy\n",
      "and can only be accounted for through a multiperiod formu-\n",
      "lation involving intricate room thermal dynamics [4].\n",
      "3) Battery charging control: This problem involves opti-\n",
      "mizing the charging and discharging cycles of a home battery\n",
      "system, considering factors such as electricity pricing, solar\n",
      "generation, and household demand, as seen in competitions\n",
      "like the CityLearn Challenge [22]. As discussed in [23],\n",
      "selecting the appropriate optimization problem for a given\n",
      "context is crucial. Unlike [23], where the context is derived\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
      "260\n",
      "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
      "from a reward signal, our context is provided by the user in\n",
      "natural language. The EC interprets this context accurately\n",
      "to establish the right optimization problem.\n",
      "EC successfully formulates an accurate objective that com-\n",
      "putes the weighted sum of total electricity cost, considering\n",
      "the electricity price. It also correctly formulates the lower\n",
      "and upper limits for the charging and discharging variables,\n",
      "the temporal interdependence of the state of charge and\n",
      "the charging rate, and the maximum capacity constraint.\n",
      "The energy balance constraint identiﬁed demonstrates LLM’s\n",
      "understanding of world modeling, including this constraint\n",
      "without speciﬁc user instruction. EC’s explanation further\n",
      "shows an understanding of physical constraints and its over-\n",
      "all objective. However, EC implicitly assumes that excess\n",
      "energy produced each hour can be sold back to the grid at\n",
      "the same rate as it was purchased, which is not typically\n",
      "the case. Future work can explore methods to encourage\n",
      "EC to be more explicit about assumptions when constructing\n",
      "optimization instances.\n",
      "B. Long-term planning for sustainability\n",
      "1) Cost-beneﬁt analysis of installing rooftop solar PVs:\n",
      "To perform a cost-beneﬁt analysis of solar PV, one must\n",
      "estimate the costs and beneﬁts over the system’s lifespan\n",
      "and compare them to a relevant alternative system [24].\n",
      "EC shows proﬁciency in understanding physical con-\n",
      "straints, such as the stipulation that the panel area shouldn’t\n",
      "surpass the roof area, and proposes that the installed area\n",
      "be adequate to supply the electricity demand. However, it\n",
      "operates under a few assumptions, such as the user planning\n",
      "to source all electricity demand from PV to fully leverage\n",
      "the budget, and that the PV’s efﬁciency stands at 0.12. While\n",
      "these assumptions are generally sensible, stating them in\n",
      "the explanations would make the model’s workings more\n",
      "transparent. Additionally, the model seems to underutilize the\n",
      "user’s provided information, such as the building’s location,\n",
      "which could potentially inform the required area-to-power\n",
      "conversion efﬁciency more accurately. This is understandable\n",
      "given the model’s lack of access to an external database,\n",
      "and future work that enhances this capability could be a\n",
      "worthwhile pursuit.\n",
      "2) Cost-beneﬁt analysis of installing a heat pump: Key\n",
      "considerations in the cost-beneﬁt analysis of a heat pump\n",
      "include the initial cost of installation, which can vary based\n",
      "on the type, size, and complexity of the system, as well as\n",
      "the availability of incentives and rebates [25].\n",
      "Similar to the PV installation scenario in Sec. III-B.1,\n",
      "EC proﬁciently identiﬁes pivotal relationships, including the\n",
      "annual operating cost of a central AC and a heat pump.\n",
      "EC’s approach focuses on optimizing annual savings, leaving\n",
      "the actual estimation of the payback period to the user.\n",
      "For instance, EC calculates that the annual savings with a\n",
      "heat pump equals $550, then elaborates on how to use this\n",
      "data for investment decisions, suggesting “If the purchase\n",
      "and installation of the heat pump cost $5,000, the payback\n",
      "period would equate to around 9.1 years ($5,000 / $550).”\n",
      "EV Charging\n",
      "Battery Charging Battery Capacity\n",
      "Solar Panel\n",
      "HVAC\n",
      "Heat Pump\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Compilation\n",
      "Correctness\n",
      "Explanation\n",
      "Problem category\n",
      "Success rate (%)\n",
      "Fig. 4: The efﬁciency of EC when integrated with SCIPY\n",
      "is gauged by the compilation success of the Python code it\n",
      "generates, the solution’s precision, and the lucidity of the\n",
      "explanation presented to the user. The black lines highlight\n",
      "the performance shifts observed when employing CVXPY.\n",
      "Fig. 5: (A) A comparative analysis of the number of itera-\n",
      "tions needed to achieve executable code across all problem\n",
      "categories (note that none of the experimental instances use\n",
      "4 iterations). (B) A comparison of the average number of\n",
      "code generations needed for EC to produce executable code,\n",
      "limited to a maximum of 5. The black bars represent the\n",
      "variation in iterations across problem categories over 20\n",
      "random runs.\n",
      "EC also suggests users compare alternatives, plan long-\n",
      "term strategies, evaluate environmental impact, and seek\n",
      "incentives or rebates. This shows that EC can provide not\n",
      "only a decision but also the context and information nec-\n",
      "essary to empower users to make more cost-effective and\n",
      "environmentally conscious choices.\n",
      "C. General ﬁndings and analysis\n",
      "To elucidate the types of errors EC commits, we display\n",
      "the success rates concerning error-free code production,\n",
      "logical correctness of the code, and the ability to articulate\n",
      "the solution derived from the optimization code to the end\n",
      "user in Fig. 4. Our framework enables EC to achieve a 100%\n",
      "compilation rate. When EC generates an error-free optimiza-\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
      "261\n",
      "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
      "tion code, it consistently explains the results to the end\n",
      "user. SCIPY outperforms CVXPY in overall performance,\n",
      "aligning with our observation that SCIPY has broader online\n",
      "recognition and use, where most LLMs receive training.\n",
      "Fig. 5 presents the average number of code regenerations\n",
      "needed and the average debugging iterations required to\n",
      "obtain syntax error-free optimization code. Although each\n",
      "subproblem within the optimization problem category ex-\n",
      "hibits different success rates, a relatively low number of code\n",
      "regenerations are typically needed. When the LLM consis-\n",
      "tently commits errors, incorporating debugging information\n",
      "in our framework is essential for achieving optimization solu-\n",
      "tions. Reducing the number of generated code samples from\n",
      "s = 5 to s = 1 slightly increases the required debugging\n",
      "iterations across all examples, suggesting that debugging is\n",
      "a more efﬁcient error-handling strategy compared to simple\n",
      "regeneration.\n",
      "Estimating the Probability of One-Round Autoformal-\n",
      "ism Success. The aforementioned results allow us to infer\n",
      "the probability of generating valid code successfully, denoted\n",
      "as p. We postulate that the event of successfully generating\n",
      "valid code is independent and identically distributed for the\n",
      "same category of problems, following a truncated geometric\n",
      "distribution with a cap at 5 trials. Based on this supposition,\n",
      "we deduce that P5\n",
      "k=1 k(1−p)k−1p+6 P\n",
      "k≥6(1−p)k−1p =\n",
      "z, where z signiﬁes the number of generations needed to\n",
      "achieve success (with generations capped at 5, as shown in\n",
      "Fig. 5), and p is the probability of success to be estimated.\n",
      "Substituting the values for z from Fig. 5 into the equation and\n",
      "solving for p, we obtain the one-round autoformalism success\n",
      "rates p = 0.8, 0.25, 0.38, 0.53, 0.83, 0.38 for the problems\n",
      "of EV Charging, Battery Charging, Battery Capacity, Solar\n",
      "Panel, HVAC Control, and Heat Pump Investment, respec-\n",
      "tively.\n",
      "Estimation of the Probability of Debugging Success.\n",
      "We posit that the event of successful debugging, given the\n",
      "presence of an erroneous code, is independent and identically\n",
      "distributed across all problem classes. Using Fig. 5, we ﬁrst\n",
      "normalize the frequency of the required number of debugging\n",
      "iterations by the frequency of generating an erroneous code\n",
      "in the ﬁrst run (i.e., 1 −0.7 = 0.3). This reveals that the\n",
      "frequencies of observing debugging iterations 1, 2, 3, 4, and\n",
      "≥5 are represented by yk, where k 2 {1, 2, ..., 5}. Let q\n",
      "denote the probability of successfully debugging the code.\n",
      "From this, we can construct a system of polynomial equa-\n",
      "tions in the form of (1 −q)k−1q = yk for k 2 {1, 2, 3, 4},\n",
      "and 1 −P4\n",
      "k=1(1 −q)k−1q = y5 given that debugging is\n",
      "capped at 5 iterations. Using a line search between 0 and 1\n",
      "in increments of 0.01, we calculate the values of ˆy1 to ˆy5\n",
      "for each q using the speciﬁed equations, and determine the\n",
      "mean squared error (MSE) between the vectors ˆy and y. Our\n",
      "optimal estimate for the probability is q = 0.26.\n",
      "Optimality Gap and Improvement Over Baseline. The\n",
      "optimality gap for a feasible solution is calculated as the ratio\n",
      "v/v⇤−1, where v denotes the objective value of the candidate\n",
      "solution and v⇤signiﬁes the optimal value of the corre-\n",
      "sponding minimization problem. In Fig. 4, we classify test\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "Problem Category\n",
      "EV Charging\n",
      "Battery Charging\n",
      "Battery Capacity\n",
      "Solar Panel\n",
      "HVAC\n",
      "Heat Pump\n",
      "Improvement over Baseline (%)\n",
      "Optimality Gap (%)\n",
      "Fig. 6: We assess the average optimality gap by compar-\n",
      "ing the exact solution of the optimization problem with\n",
      "the solution determined by EC, alongside evaluating the\n",
      "enhancement over the baseline in terms of optimality.\n",
      "cases as correct only if they equate to the globally optimal\n",
      "solution value, i.e., a 0% optimality gap. Additional results\n",
      "presented in Fig. 6 demonstrate the average optimality gap\n",
      "when incorrect logic within the code results in the omission\n",
      "of key parameters (such as the efﬁciency of a battery). Our\n",
      "framework was evaluated using 20 examples for each energy\n",
      "optimization problem, including EV charging. As depicted,\n",
      "the instances display an optimality gap generally within the\n",
      "20% range, with the majority under 10%.\n",
      "For the baseline method, the model was simply prompted\n",
      "with our question along with necessary parameters to solve\n",
      "the optimization problem. We noticed that even when the\n",
      "problem is clearly an optimization issue, LLMs may opt\n",
      "to generate responses by attempting to apply logic towards\n",
      "reaching an answer. As seen in Fig. 6, this approach does not\n",
      "yield favorable results in comparison to the EC framework.\n",
      "This can be attributed to the fact that many energy-related\n",
      "optimization problems of importance do not yield closed-\n",
      "form solutions, making the correct utilization of convex\n",
      "program solvers critical.\n",
      "We further report on the improvement over the baseline,\n",
      "measured by vb/v −1, where vb represents the objective\n",
      "value of the baseline solution. As is evident, the improvement\n",
      "can amount to as much as 60%, with most instances falling\n",
      "within the 30% range. Some of the problems yielding the\n",
      "greatest improvements include Solar Panel, Heat Pump,\n",
      "Battery Charging, and EV Charging, which encompass both\n",
      "real-time decisions and long-term investments.\n",
      "IV. DISCUSSIONS AND CONCLUSION\n",
      "Conversational AI for sustainability has long seemed\n",
      "aspirational, but LLMs offer new potential. By providing\n",
      "an intuitive interface, our framework helps individuals en-\n",
      "gage more consciously about energy use. Greater awareness\n",
      "drives sustainable behaviors like responsible consumption,\n",
      "efﬁciency, and solar adoption [26], [27].\n",
      "Households average 3.5 kWh peak electricity use, while\n",
      "EV chargers approach 10 kWh. EV charging at peak triples\n",
      "load, straining infrastructure. Utilities offer off-peak rates,\n",
      "but ﬁnancial incentives alone don’t fully shift behaviors—\n",
      "an “efﬁciency gap” phenomenon [26], [27]. Our framework\n",
      "simpliﬁes this via conversation. With 2022’s 3 million EVs,\n",
      "adopting off-peak charging could save $876 million annually\n",
      "(at $0.06 vs $0.14 per kWh rates) [28], [29]. With 2035’s\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
      "262\n",
      "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
      "projected 73 million EVs [30], savings could reach $21.3\n",
      "billion. Beyond consumer savings, off-peak charging lessens\n",
      "grid stress, stabilizes prices, and reduces needs for new\n",
      "plants. By simplifying optimization, our system enables\n",
      "impactful sustainability actions.\n",
      "A key limitation of EC is its reliance on the LLM’s\n",
      "ability to accurately formulate optimization problems, ensur-\n",
      "ing proper alignment of objectives and constraints. Incorrect\n",
      "formulations can lead to ineffective or harmful solutions,\n",
      "despite the perceived reliability of these systems. Auto-\n",
      "informalism (Sec. II-A.3) helps mitigate this risk by scru-\n",
      "tinizing problem formulations more carefully and alerting\n",
      "users to potential issues or assumptions. Enhancing auto-\n",
      "informalism’s effectiveness is thus critical for reliable solu-\n",
      "tions. Additional reliability methods like validation routines,\n",
      "robustness testing, and expanded user feedback loops can\n",
      "further strengthen the framework.\n",
      "While we merely sketch out the potential, the pro-\n",
      "posed shift towards human-guided optimization autoformal-\n",
      "ism could democratize access to sophisticated technologies\n",
      "and set the stage for a more equitable and sustainable future.\n",
      "REFERENCES\n",
      "[1] S. M. Arif, T. T. Lie, B. C. Seet, S. Ayyadi, and K. Jensen, “Review\n",
      "of electric vehicle technologies, charging methods, standards and\n",
      "optimization techniques,” Electronics, vol. 10, no. 16, p. 1910, 2021.\n",
      "[2] R. Khezri, A. Mahmoudi, and H. Aki, “Optimal planning of solar\n",
      "photovoltaic and battery storage systems for grid-connected residential\n",
      "sector: Review, challenges and new perspectives,” Renewable and\n",
      "Sustainable Energy Reviews, vol. 153, p. 111763, 2022.\n",
      "[3] Y. Yang, S. Bremner, C. Menictas, and M. Kay, “Battery energy\n",
      "storage system size determination in renewable energy systems: A\n",
      "review,” Renewable and Sustainable Energy Reviews, vol. 91, pp. 109–\n",
      "125, 2018.\n",
      "[4] J. Drgoˇna, J. Arroyo, I. C. Figueroa, D. Blum, K. Arendt, D. Kim,\n",
      "E. P. Oll´e, J. Oravec, M. Wetter, D. L. Vrabie et al., “All you need to\n",
      "know about model predictive control for buildings,” Annual Reviews\n",
      "in Control, vol. 50, pp. 190–232, 2020.\n",
      "[5] B. P. Esther and K. S. Kumar, “A survey on residential demand\n",
      "side management architecture, approaches, optimization models and\n",
      "methods,” Renewable and Sustainable Energy Reviews, vol. 59, pp.\n",
      "342–351, 2016.\n",
      "[6] J. Currie, D. I. Wilson, N. Sahinidis, and J. Pinto, “Opti: Lowering the\n",
      "barrier between open source optimizers and the industrial matlab user,”\n",
      "Foundations of computer-aided process operations, vol. 24, p. 32,\n",
      "2012.\n",
      "[7] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\n",
      "J. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\n",
      "models: Methods, analysis & insights from training gopher,” arXiv\n",
      "preprint arXiv:2112.11446, 2021.\n",
      "[8] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V.\n",
      "Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in\n",
      "large language models,” in Advances in Neural Information Processing\n",
      "Systems, 2022.\n",
      "[9] B. Sel, A. Al-Tawaha, v Vanshaj, R. Jia, and M. Jin, “Algorithm of\n",
      "thoughts: Enhancing exploration of ideas in large language models,”\n",
      "ICML, 2024.\n",
      "[10] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. E. Staats, M. Jamnik,\n",
      "and C. Szegedy, “Autoformalization with large language models,” in\n",
      "Advances in Neural Information Processing Systems, 2022.\n",
      "[11] Y. Wu, A. Q. Jiang, W. Li, M. Rabe, C. Staats, M. Jamnik, and\n",
      "C. Szegedy, “Autoformalization with large language models,” Ad-\n",
      "vances in Neural Information Processing Systems, vol. 35, pp. 32 353–\n",
      "32 368, 2022.\n",
      "[12] A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd, “A rewriting\n",
      "system for convex optimization problems,” Journal of Control and\n",
      "Decision, vol. 5, no. 1, pp. 42–60, 2018.\n",
      "[13] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\n",
      "E. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\n",
      "language models,” arXiv preprint arXiv:2108.07732, 2021.\n",
      "[14] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V.\n",
      "Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in\n",
      "large language models,” Advances in Neural Information Processing\n",
      "Systems, vol. 35, pp. 24 824–24 837, 2022.\n",
      "[15] G. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\n",
      "R. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\n",
      "maz et al., “Augmented language models: a survey,” arXiv preprint\n",
      "arXiv:2302.07842, 2023.\n",
      "[16] T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama,\n",
      "“Demystifying gpt self-repair for code generation,” arXiv preprint\n",
      "arXiv:2306.09896, 2023.\n",
      "[17] A. B. Saka, L. O. Oyedele, L. A. Akanbi, S. A. Ganiyu, D. W.\n",
      "Chan, and S. A. Bello, “Conversational artiﬁcial intelligence in the\n",
      "aec industry: A review of present status, challenges and opportunities,”\n",
      "Advanced Engineering Informatics, vol. 55, p. 101869, 2023.\n",
      "[18] R. Panchalingam and K. C. Chan, “A state-of-the-art review on\n",
      "artiﬁcial intelligence for smart buildings,” Intelligent Buildings Inter-\n",
      "national, vol. 13, no. 4, pp. 203–226, 2021.\n",
      "[19] M. Jin, V. Khattar, H. Kaushik, B. Sel, and R. Jia, “On solution func-\n",
      "tions of optimization: Universal approximation and covering number\n",
      "bounds,” AAAI, 2023.\n",
      "[20] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing\n",
      "source code using a neural attention model,” in Proceedings of the\n",
      "54th Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers), 2016, pp. 2073–2083.\n",
      "[21] M. Jin, B. Sel, H. FNU, and W. Yin, “Democratizing energy manage-\n",
      "ment with llm-assisted optimization autoformalism,” 2024, full Ver-\n",
      "sion:\n",
      "http://www.jinming.tech/papers/ChatEnergy23-smartgridcomm.\n",
      "pdf.\n",
      "[22] J. R. V´azquez-Canteli, J. K¨ampf, G. Henze, and Z. Nagy, “Citylearn\n",
      "v1.0: An openai gym environment for demand response with deep\n",
      "reinforcement learning,” in Proceedings of the 6th ACM International\n",
      "Conference on Systems for Energy-Efﬁcient Buildings, Cities, and\n",
      "Transportation, ser. BuildSys ’19.\n",
      "New York, NY, USA: Association\n",
      "for Computing Machinery, 2019, p. 356–357. [Online]. Available:\n",
      "https://doi.org/10.1145/3360322.3360998\n",
      "[23] V. Khattar and M. Jin, “Winning the citylearn challenge: Adaptive opti-\n",
      "mization with evolutionary search under trajectory-based guidance,” in\n",
      "Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 37,\n",
      "no. 12, 2023, pp. 14 286–14 294.\n",
      "[24] M. Thebault and L. Gaillard, “Optimization of the integration of\n",
      "photovoltaic systems on buildings for self-consumption–case study in\n",
      "france,” City and Environment Interactions, vol. 10, p. 100057, 2021.\n",
      "[25] F. Bela¨ıd, Z. Ranjbar, and C. Massi´e, “Exploring the cost-effectiveness\n",
      "of energy efﬁciency implementation measures in the residential sector,”\n",
      "Energy Policy, vol. 150, p. 112122, 2021.\n",
      "[26] O. I. Asensio and M. A. Delmas, “Nonprice incentives and energy\n",
      "conservation,” Proceedings of the National Academy of Sciences, vol.\n",
      "112, no. 6, pp. E510–E515, 2015.\n",
      "[27] T. D. Gerarden, R. G. Newell, and R. N. Stavins, “Assessing the\n",
      "energy-efﬁciency gap,” Journal of economic literature, vol. 55, no. 4,\n",
      "pp. 1486–1525, 2017.\n",
      "[28] APPALACHIAN\n",
      "POWER,\n",
      "“Virginia\n",
      "s.c.c.\n",
      "tariff\n",
      "no.\n",
      "26\n",
      "appalachian\n",
      "power\n",
      "company,”\n",
      "2021.\n",
      "[Online].\n",
      "Available: https://www.appalachianpower.com/lib/docs/ratesandtariffs/\n",
      "Virginia/Tariff26-MASTER-Standard-June1-2023RPS-RAC.pdf\n",
      "[29] IEA,\n",
      "“Trends\n",
      "in\n",
      "electric\n",
      "light-duty\n",
      "vehicles,”\n",
      "2023.\n",
      "[On-\n",
      "line]. Available: https://www.iea.org/reports/global-ev-outlook-2023/\n",
      "trends-in-electric-light-duty-vehicles\n",
      "[30] NREL, “National economic value assessment of plug-in electric\n",
      "vehicles,”\n",
      "2016.\n",
      "[Online].\n",
      "Available:\n",
      "https://www.nrel.gov/docs/\n",
      "fy17osti/66980.pdf\n",
      "[31] H. C. Hesse, R. Martins, P. Musilek, M. Naumann, C. N. Truong, and\n",
      "A. Jossen, “Economic optimization of component sizing for residential\n",
      "battery storage systems,” Energies, vol. 10, no. 7, p. 835, 2017.\n",
      "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
      "263\n",
      "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_top_k_similar_indices(query_vector, abstract_vectors, k):\n",
    "    \n",
    "    #Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n",
    "    \n",
    "    #Parameters:\n",
    "    #- query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n",
    "    #- abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n",
    "    #- k: The number of top indices to return.\n",
    "    \n",
    "    #Returns:\n",
    "    #- sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n",
    "    cosines = cosine_similarity(query_vector.cpu().detach().numpy(), abstract_vectors.cpu().detach().numpy())\n",
    "    sorted_indices = np.argsort(cosines, axis=1)[:, ::-1] #inverse\n",
    "\n",
    "    return sorted_indices[:, :k]\n",
    "\n",
    "\n",
    "def retrieve_documents(indices, documents_dict):\n",
    "    \n",
    "    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n",
    "    \n",
    "    #Parameters:\n",
    "    #- indices: A numpy array or list of top-k indices of the most similar documents.\n",
    "    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n",
    "    \n",
    "    #Returns:\n",
    "    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n",
    "\n",
    "\n",
    "    text_list = [documents_dict[f'paper{i}'] for i in indices[0]]\n",
    "    text= ' '.join(text_list)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "#now I create a vector also for my query \n",
    "\n",
    "query = \"energy topics covered by papers with large language models\"\n",
    "\n",
    "inputs = tokenizer(query, return_tensors='pt', padding=True)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "query_vector = model(**inputs).last_hidden_state[:, 0]\n",
    "\n",
    "print(f\"Model is on device: {model.device}\")\n",
    "print(f\"Query tensor is on device: {inputs['input_ids'].device}\")\n",
    "\n",
    "indices = get_top_k_similar_indices(query_vector, abstract_vectors, 1)\n",
    "\n",
    "print(f\"BEST DOCUMENTS: {indices}\")\n",
    "concatenated_documents=retrieve_documents(indices, pdf_texts)\n",
    "print(concatenated_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to perform Retrieval Augmented Generation\n",
    "\n",
    "In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]\n",
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Provide straight to the point answers, without bullet points.\n",
      "<</SYS>>\n",
      "Question: What are the main energy issues for Large Language Models?. Collect information from the following article: Offline Energy-Optimal LLM Serving: Workload-Based Energy\n",
      "Models for LLM Inference on Heterogeneous Systems\n",
      "Grant Wilkins\n",
      "gfw27@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Srinivasan Keshav\n",
      "sk818@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Richard Mortier\n",
      "rmm1002@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "ABSTRACT\n",
      "The rapid adoption of large language models (LLMs) has led to\n",
      "significant advances in natural language processing and text gener-\n",
      "ation. However, the energy consumed through LLM model infer-\n",
      "ence remains a major challenge for sustainable AI deployment. To\n",
      "address this problem, we model the workload-dependent energy\n",
      "consumption and runtime of LLM inference tasks on heterogeneous\n",
      "GPU-CPU systems. By conducting an extensive characterization\n",
      "study of several state-of-the-art LLMs and analyzing their energy\n",
      "and runtime behavior across different magnitudes of input prompts\n",
      "and output text, we develop accurate (𝑅2 > 0.96) energy and run-\n",
      "time models for each LLM. We employ these models to explore\n",
      "an offline, energy-optimal LLM workload scheduling framework.\n",
      "Through a case study, we demonstrate the advantages of energy\n",
      "and accuracy aware scheduling compared to existing best practices.\n",
      "CCS CONCEPTS\n",
      "• Computer systems organization →Heterogeneous (hybrid)\n",
      "systems; • Hardware →Impact on the environment.\n",
      "KEYWORDS\n",
      "Sustainable computing, Heterogeneous computing, Large Language\n",
      "Models, Artificial Intelligence\n",
      "ACM Reference Format:\n",
      "Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Offline Energy-\n",
      "Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on\n",
      "Heterogeneous Systems. In The 3rd ACM HotCarbon Workshop on Sustainable\n",
      "Computer System (HotCarbon ’24), 9 July 2024, Santa Cruz, CA. ACM, New\n",
      "York, NY, USA, 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n",
      "1\n",
      "INTRODUCTION\n",
      "Rapid advancements in large language models (LLMs) have rev-\n",
      "olutionized natural language processing, enabling AI systems to\n",
      "achieve human-level performance on a wide range of language\n",
      "tasks [3, 26, 40]. However, the computational resources and energy\n",
      "consumption associated with deploying these models present signif-\n",
      "icant challenges to not only energy systems but also sustainability\n",
      "goals [20, 21, 29]. As LLMs become increasingly integrated into\n",
      "Permission to make digital or hard copies of part or all of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for third-party components of this work must be honored.\n",
      "For all other uses, contact the owner/author(s).\n",
      "HotCarbon’24, 9 July 2024, Santa Cruz, CA\n",
      "© 2024 Copyright held by the owner/author(s).\n",
      "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM\n",
      "https://doi.org/10.1145/nnnnnnn.nnnnnnn\n",
      "real-world applications, optimizing their energy efficiency during\n",
      "inference is crucial for sustainable AI development [45].\n",
      "Inference, the process of using a trained model to make predic-\n",
      "tions on new data, is a critical phase in LLM deployment as it is the\n",
      "point at which AI capabilities become accessible to users. Unlike the\n",
      "one-time training process, inference is an ongoing, real-time pro-\n",
      "cess that directly impacts end-user experience. Inference in LLMs\n",
      "can be computationally expensive due to model size [45] and quality\n",
      "of service/latency expectations [42]. Scaling LLMs up across large\n",
      "data centers is challenging due to power [27] and communication\n",
      "overheads [28].\n",
      "The energy intensity of LLM inference can be substantial even\n",
      "when compared to training [4]. Decarbonizing the energy sources\n",
      "for data centers can be challenging due to both sporadic demand and\n",
      "regional inefficiencies in adopting renewables. Higher energy con-\n",
      "sumption of an application approximately correlates with greater\n",
      "carbon intensity [32]. It is thus crucial to find energy-efficient meth-\n",
      "ods to mitigate the environmental costs of LLM inference [1, 18].\n",
      "To address this issue, we propose a workload-based model of\n",
      "energy consumption for LLM inference to let system operators\n",
      "navigate the trade-off between accuracy and energy usage.\n",
      "Our contributions are as follows:\n",
      "(1) We characterize the energy consumption and runtime be-\n",
      "havior of several state-of-the-art LLMs on a heterogeneous\n",
      "GPU-CPU system (§4).\n",
      "(2) We develop workload-based energy and runtime models\n",
      "that accurately capture the relationship between the number\n",
      "of input and output tokens and the energy and runtime\n",
      "characteristics of each LLM (§5).\n",
      "(3) We demonstrate the effectiveness of our approach through a\n",
      "case study, showcasing a tunable trade-off between energy\n",
      "and accuracy (§6).\n",
      "Our profiling framework and datasets are openly available.1\n",
      "2\n",
      "RELATED WORK\n",
      "2.1\n",
      "Energy Consumption in AI Systems\n",
      "Recent reports have found that the computation required by state-\n",
      "of-the-art AI systems entail massive energy consumption and car-\n",
      "bon emissions [4, 22, 29, 35, 45]. The energy intensity of AI systems\n",
      "can be broadly split between the energy required for training and\n",
      "that required for inference after models are deployed [10]. Training\n",
      "complex models on massive datasets is an energy-intensive process,\n",
      "with estimates finding that training GPT-3 required 1,287 megawatt-\n",
      "hours of energy [29]. Even with this huge amount of energy, a year\n",
      "of inference by an LLM on cloud infrastructure can consume over\n",
      "1https://github.com/grantwilkins/energy-inference.git\n",
      "arXiv:2407.04014v1  [cs.DC]  4 Jul 2024\n",
      "HotCarbon’24, 9 July 2024, Santa Cruz, CA\n",
      "Grant Wilkins, Srinivasan Keshav, and Richard Mortier\n",
      "25× more energy than training that same model [4]. Some of these\n",
      "issues and emissions of course depend on the deployment scale and\n",
      "hardware efficiency [35], however, the trend remains that energy\n",
      "consumption in inference is a large issue. Optimizing software and\n",
      "hardware specifically for AI workloads is thus essential [1].\n",
      "Desislavov et al. [5] provide an examination of trends in AI in-\n",
      "ference energy consumption \n",
      "Answer: What are the main energy issues for Large Language Models?\n",
      "Collect information from the following article: Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems\n",
      "Models for LLM Inference on Heterogeneous Systems\n",
      "Grant Wilkins\n",
      "gfw27@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Srinivasan Keshav\n",
      "sk818@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Richard Mortier\n",
      "rmm1002@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "ABSTRACT\n",
      "The rapid adoption of large language models (LLMs) has led to\n",
      "significant advances in natural language processing and text gener-\n",
      "ation. However, the energy consumed through LLM model infer-\n",
      "ence remains a major challenge for sustainable AI deployment. To\n",
      "address this problem, we model the workload-dependent energy\n",
      "consumption and runtime of LLM inference tasks on heterogeneous\n",
      "GPU-CPU systems. By conducting an extensive characterization\n",
      "study of several state-of-the-art LLMs and analyzing their energy\n",
      "and runtime behavior across different magnitudes of input prompts\n",
      "and output text, we develop accurate (𝑅2 > 0.96) energy and run-\n",
      "time models for each LLM. We employ these models to explore\n",
      "an offline, energy-optimal LLM workload scheduling framework.\n",
      "Through a case study, we demonstrate the advantages of energy\n",
      "and accuracy aware scheduling compared\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#now we put it all together\n",
    "\n",
    "def generate_augmented_response(query, documents):\n",
    "    modelBERT = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizerBERT = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    modelBERT.to(device)\n",
    "\n",
    "\n",
    "    if tokenizerBERT.pad_token_id is None:\n",
    "        tokenizerBERT.pad_token_id = tokenizerBERT.eos_token_id\n",
    "\n",
    "    system = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    Provide straight to the point answers, without bullet points.\n",
    "<</SYS>>\"\"\"\n",
    "\n",
    "    query_for_rag = query    \n",
    "    inputs = tokenizerBERT(query, return_tensors='pt', padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    query_vector = modelBERT(**inputs).last_hidden_state[:, 0]\n",
    "    indices = get_top_k_similar_indices(query_vector, abstract_vectors, 1)\n",
    "\n",
    "\n",
    "\n",
    "    context = retrieve_documents(indices, documents)[:6000] #the context is the entire document that matches the query\n",
    "\n",
    "    \n",
    "    prompt =  \"\"\"{SYS}\\nQuestion: {PROMPT}. Collect information from the following article: {BODY} \\nAnswer:\"\"\".format(SYS=system, PROMPT=query_for_rag, BODY=context)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    outputs = model.generate(inputs['input_ids'],\n",
    "                            attention_mask = inputs['attention_mask'],\n",
    "                            max_new_tokens=300,\n",
    "                            max_length=500,\n",
    "                            top_k=10,\n",
    "                            num_return_sequences=1,\n",
    "                            temperature=0.7,\n",
    "                            repetition_penalty=1.0,\n",
    "                            length_penalty=1.0,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            pad_token_id = tokenizer.pad_token_id,\n",
    "                            early_stopping=True)\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "query = \"What are the main energy issues for Large Language Models?\"\n",
    "response = generate_augmented_response(query, pdf_texts)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
