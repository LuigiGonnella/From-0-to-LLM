{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21052d8a",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "**Inception module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed52091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0567ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.conv_1x1 = nn.Conv2d(192, 64, 1)\n",
    "        self.conv_1x1_3x3 = nn.Conv2d(192, 96, 1)\n",
    "        self.conv_1x1_5x5 = nn.Conv2d(192, 16, 1)\n",
    "        self.max_pool = nn.MaxPool2d(3, 1, 1)\n",
    "        self.conv_pool = nn.Conv2d(192, 32, 1)\n",
    "\n",
    "        self.conv_3x3 = nn.Conv2d(96, 128, 3, 1, 1)\n",
    "        self.conv_5x5 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        in_3x3 = self.conv_1x1_3x3(x)\n",
    "        in_5x5 = self.conv_1x1_5x5(x)\n",
    "        in_1x1 = self.max_pool(x)\n",
    "\n",
    "        in_cat = self.conv_1x1(x) #64x28x28\n",
    "        out_3x3 = self.conv_3x3(in_3x3) #128x28x28\n",
    "        out_5x5 = self.conv_5x5(in_5x5) #32x28x28\n",
    "        out_pool = self.conv_pool(in_1x1) #32x28x28\n",
    "\n",
    "        out = torch.cat([in_cat, out_3x3, out_5x5, out_pool], dim=1) #cat across channels\n",
    "        #(64 + 128 +32 + 32)x28x28 = 1x256x28x28\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3614b9",
   "metadata": {},
   "source": [
    "let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243415d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "inception = InceptionModule()\n",
    "\n",
    "sample = torch.randn(1, 192, 28, 28) #torch has BxCxHxW convention\n",
    "\n",
    "out = inception(sample)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec8b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 28, 28]          18,528\n",
      "            Conv2d-2           [-1, 16, 28, 28]           3,088\n",
      "         MaxPool2d-3          [-1, 192, 28, 28]               0\n",
      "            Conv2d-4           [-1, 64, 28, 28]          12,352\n",
      "            Conv2d-5          [-1, 128, 28, 28]         110,720\n",
      "            Conv2d-6           [-1, 32, 28, 28]          12,832\n",
      "            Conv2d-7           [-1, 32, 28, 28]           6,176\n",
      "================================================================\n",
      "Total params: 163,696\n",
      "Trainable params: 163,696\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3.35\n",
      "Params size (MB): 0.62\n",
      "Estimated Total Size (MB): 4.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(inception, (192, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80467bb1",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "**NN for ECG interpretation**\n",
    "Convolutional neural networks can be applied\n",
    "to one-dimensional as well as two- or three-\n",
    "dimensional input. Let us implement a\n",
    "residual neural network that was proposed\n",
    "for ECG interpretation1. The network is\n",
    "trained on 30 second long signals sampled at\n",
    "200Hz --> **(1, 1, 6000) INPUT SAMPLES**. The architecture is depicted in the\n",
    "figure to the right.\n",
    "- The convolutional layers all have a filter\n",
    "length of 16 and have 64. Every alternate\n",
    "residual block subsamples its inputs by a\n",
    "factor of 2, thus the original input is\n",
    "ultimately subsampled by a factor of 2^8 --> 256.\n",
    "- When a residual block subsamples the input,\n",
    "the corresponding shortcut connections also\n",
    "subsample their input using a Max Pooling\n",
    "operation with the same subsample factor.\n",
    "- The final fully connected layer and softmax\n",
    "activation produce a distribution over the 14\n",
    "output classes for each time-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2142fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32debba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleResidualBlock(nn.Module):\n",
    "    def __init__(self, index):\n",
    "        super(SingleResidualBlock, self).__init__()\n",
    "        stride = 2 if index % 2 == 0 else 1\n",
    "        padding = 0 if stride == 2 else 1\n",
    "        k_size = 2 if stride == 2 else 3\n",
    "\n",
    "        left = (16 - 1) // 2      # 7\n",
    "        right = (16 - 1) - left   # 8\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConstantPad1d((left, right), 0),\n",
    "            nn.Conv1d(64, 64, 16, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # conv2: 64 -> 64, may subsample with stride\n",
    "        if stride == 2:\n",
    "            self.conv2 = nn.Conv1d(64, 64, 16, stride=stride, padding=7)\n",
    "        else:\n",
    "           self.conv2 = nn.Sequential(\n",
    "            nn.ConstantPad1d((left, right), 0),\n",
    "            nn.Conv1d(64, 64, 16, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(64) #batch norm wants only #channels\n",
    "        self.dp = nn.Dropout1d(p=0.7)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=k_size, stride=stride, padding=padding)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn(self.conv1(x)))\n",
    "        x1 = self.dp(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "\n",
    "        max_pool = self.pool(x)\n",
    "        \n",
    "        out = max_pool + x1\n",
    "        return out \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b9496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedResidualBlock(nn.Module):\n",
    "    def __init__(self, index):\n",
    "        super(RepeatedResidualBlock, self).__init__()\n",
    "        stride = 2 if index % 2 == 0 else 1\n",
    "        padding = 0 if stride == 2 else 1\n",
    "        k_size = 2 if stride == 2 else 3\n",
    "\n",
    "        left = (16 - 1) // 2      # 7\n",
    "        right = (16 - 1) - left   # 8\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64) #batch norm wants only #channels\n",
    "        self.dp = nn.Dropout1d(p=0.7)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConstantPad1d((left, right), 0),\n",
    "            nn.Conv1d(64, 64, 16, stride=1, padding=0)\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(64) #batch norm wants only #channels\n",
    "        \n",
    "         # conv2: 64 -> 64, may subsample with stride\n",
    "        if stride == 2:\n",
    "            self.conv2 = nn.Conv1d(64, 64, 16, stride=stride, padding=7)\n",
    "        else:\n",
    "           self.conv2 = nn.Sequential(\n",
    "            nn.ConstantPad1d((left, right), 0),\n",
    "            nn.Conv1d(64, 64, 16, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=k_size, stride=stride, padding=padding)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.dp(F.relu(self.bn1(x)))\n",
    "        \n",
    "        x1 = self.conv1(x1)\n",
    "\n",
    "        x1 = self.dp(F.relu(self.bn2(x1)))\n",
    "\n",
    "        x1 = self.conv2(x1)\n",
    "\n",
    "        max_pool = self.pool(x)\n",
    "        \n",
    "        out = max_pool + x1\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00103359",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4dc0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGNet(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ECGNet, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, 64, 16, stride=1)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.single_res = SingleResidualBlock(index=0)\n",
    "        self.repeated_res = nn.ModuleList([RepeatedResidualBlock(index) for index in range(1, 16)])\n",
    "\n",
    "        self.fcl = nn.Linear(64*23, NUM_CLASSES)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "        x = self.single_res(x)\n",
    "\n",
    "        for res in self.repeated_res:\n",
    "            x = res(x)\n",
    "        \n",
    "        x = F.relu(self.bn(x))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        out = self.fcl(x)\n",
    "\n",
    "        logits = F.softmax(out, dim=1)\n",
    "        \n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d09402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "model = ECGNet(in_channels=1)\n",
    "x = torch.randn(1, 1, 6000)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e00093f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 5985]           1,088\n",
      "       BatchNorm1d-2             [-1, 64, 5985]             128\n",
      "     ConstantPad1d-3             [-1, 64, 6000]               0\n",
      "            Conv1d-4             [-1, 64, 5985]          65,600\n",
      "       BatchNorm1d-5             [-1, 64, 5985]             128\n",
      "         Dropout1d-6             [-1, 64, 5985]               0\n",
      "            Conv1d-7             [-1, 64, 2992]          65,600\n",
      "         MaxPool1d-8             [-1, 64, 2992]               0\n",
      "SingleResidualBlock-9             [-1, 64, 2992]               0\n",
      "      BatchNorm1d-10             [-1, 64, 2992]             128\n",
      "        Dropout1d-11             [-1, 64, 2992]               0\n",
      "    ConstantPad1d-12             [-1, 64, 3007]               0\n",
      "           Conv1d-13             [-1, 64, 2992]          65,600\n",
      "      BatchNorm1d-14             [-1, 64, 2992]             128\n",
      "        Dropout1d-15             [-1, 64, 2992]               0\n",
      "    ConstantPad1d-16             [-1, 64, 3007]               0\n",
      "           Conv1d-17             [-1, 64, 2992]          65,600\n",
      "        MaxPool1d-18             [-1, 64, 2992]               0\n",
      "RepeatedResidualBlock-19             [-1, 64, 2992]               0\n",
      "      BatchNorm1d-20             [-1, 64, 2992]             128\n",
      "        Dropout1d-21             [-1, 64, 2992]               0\n",
      "    ConstantPad1d-22             [-1, 64, 3007]               0\n",
      "           Conv1d-23             [-1, 64, 2992]          65,600\n",
      "      BatchNorm1d-24             [-1, 64, 2992]             128\n",
      "        Dropout1d-25             [-1, 64, 2992]               0\n",
      "           Conv1d-26             [-1, 64, 1496]          65,600\n",
      "        MaxPool1d-27             [-1, 64, 1496]               0\n",
      "RepeatedResidualBlock-28             [-1, 64, 1496]               0\n",
      "      BatchNorm1d-29             [-1, 64, 1496]             128\n",
      "        Dropout1d-30             [-1, 64, 1496]               0\n",
      "    ConstantPad1d-31             [-1, 64, 1511]               0\n",
      "           Conv1d-32             [-1, 64, 1496]          65,600\n",
      "      BatchNorm1d-33             [-1, 64, 1496]             128\n",
      "        Dropout1d-34             [-1, 64, 1496]               0\n",
      "    ConstantPad1d-35             [-1, 64, 1511]               0\n",
      "           Conv1d-36             [-1, 64, 1496]          65,600\n",
      "        MaxPool1d-37             [-1, 64, 1496]               0\n",
      "RepeatedResidualBlock-38             [-1, 64, 1496]               0\n",
      "      BatchNorm1d-39             [-1, 64, 1496]             128\n",
      "        Dropout1d-40             [-1, 64, 1496]               0\n",
      "    ConstantPad1d-41             [-1, 64, 1511]               0\n",
      "           Conv1d-42             [-1, 64, 1496]          65,600\n",
      "      BatchNorm1d-43             [-1, 64, 1496]             128\n",
      "        Dropout1d-44             [-1, 64, 1496]               0\n",
      "           Conv1d-45              [-1, 64, 748]          65,600\n",
      "        MaxPool1d-46              [-1, 64, 748]               0\n",
      "RepeatedResidualBlock-47              [-1, 64, 748]               0\n",
      "      BatchNorm1d-48              [-1, 64, 748]             128\n",
      "        Dropout1d-49              [-1, 64, 748]               0\n",
      "    ConstantPad1d-50              [-1, 64, 763]               0\n",
      "           Conv1d-51              [-1, 64, 748]          65,600\n",
      "      BatchNorm1d-52              [-1, 64, 748]             128\n",
      "        Dropout1d-53              [-1, 64, 748]               0\n",
      "    ConstantPad1d-54              [-1, 64, 763]               0\n",
      "           Conv1d-55              [-1, 64, 748]          65,600\n",
      "        MaxPool1d-56              [-1, 64, 748]               0\n",
      "RepeatedResidualBlock-57              [-1, 64, 748]               0\n",
      "      BatchNorm1d-58              [-1, 64, 748]             128\n",
      "        Dropout1d-59              [-1, 64, 748]               0\n",
      "    ConstantPad1d-60              [-1, 64, 763]               0\n",
      "           Conv1d-61              [-1, 64, 748]          65,600\n",
      "      BatchNorm1d-62              [-1, 64, 748]             128\n",
      "        Dropout1d-63              [-1, 64, 748]               0\n",
      "           Conv1d-64              [-1, 64, 374]          65,600\n",
      "        MaxPool1d-65              [-1, 64, 374]               0\n",
      "RepeatedResidualBlock-66              [-1, 64, 374]               0\n",
      "      BatchNorm1d-67              [-1, 64, 374]             128\n",
      "        Dropout1d-68              [-1, 64, 374]               0\n",
      "    ConstantPad1d-69              [-1, 64, 389]               0\n",
      "           Conv1d-70              [-1, 64, 374]          65,600\n",
      "      BatchNorm1d-71              [-1, 64, 374]             128\n",
      "        Dropout1d-72              [-1, 64, 374]               0\n",
      "    ConstantPad1d-73              [-1, 64, 389]               0\n",
      "           Conv1d-74              [-1, 64, 374]          65,600\n",
      "        MaxPool1d-75              [-1, 64, 374]               0\n",
      "RepeatedResidualBlock-76              [-1, 64, 374]               0\n",
      "      BatchNorm1d-77              [-1, 64, 374]             128\n",
      "        Dropout1d-78              [-1, 64, 374]               0\n",
      "    ConstantPad1d-79              [-1, 64, 389]               0\n",
      "           Conv1d-80              [-1, 64, 374]          65,600\n",
      "      BatchNorm1d-81              [-1, 64, 374]             128\n",
      "        Dropout1d-82              [-1, 64, 374]               0\n",
      "           Conv1d-83              [-1, 64, 187]          65,600\n",
      "        MaxPool1d-84              [-1, 64, 187]               0\n",
      "RepeatedResidualBlock-85              [-1, 64, 187]               0\n",
      "      BatchNorm1d-86              [-1, 64, 187]             128\n",
      "        Dropout1d-87              [-1, 64, 187]               0\n",
      "    ConstantPad1d-88              [-1, 64, 202]               0\n",
      "           Conv1d-89              [-1, 64, 187]          65,600\n",
      "      BatchNorm1d-90              [-1, 64, 187]             128\n",
      "        Dropout1d-91              [-1, 64, 187]               0\n",
      "    ConstantPad1d-92              [-1, 64, 202]               0\n",
      "           Conv1d-93              [-1, 64, 187]          65,600\n",
      "        MaxPool1d-94              [-1, 64, 187]               0\n",
      "RepeatedResidualBlock-95              [-1, 64, 187]               0\n",
      "      BatchNorm1d-96              [-1, 64, 187]             128\n",
      "        Dropout1d-97              [-1, 64, 187]               0\n",
      "    ConstantPad1d-98              [-1, 64, 202]               0\n",
      "           Conv1d-99              [-1, 64, 187]          65,600\n",
      "     BatchNorm1d-100              [-1, 64, 187]             128\n",
      "       Dropout1d-101              [-1, 64, 187]               0\n",
      "          Conv1d-102               [-1, 64, 93]          65,600\n",
      "       MaxPool1d-103               [-1, 64, 93]               0\n",
      "RepeatedResidualBlock-104               [-1, 64, 93]               0\n",
      "     BatchNorm1d-105               [-1, 64, 93]             128\n",
      "       Dropout1d-106               [-1, 64, 93]               0\n",
      "   ConstantPad1d-107              [-1, 64, 108]               0\n",
      "          Conv1d-108               [-1, 64, 93]          65,600\n",
      "     BatchNorm1d-109               [-1, 64, 93]             128\n",
      "       Dropout1d-110               [-1, 64, 93]               0\n",
      "   ConstantPad1d-111              [-1, 64, 108]               0\n",
      "          Conv1d-112               [-1, 64, 93]          65,600\n",
      "       MaxPool1d-113               [-1, 64, 93]               0\n",
      "RepeatedResidualBlock-114               [-1, 64, 93]               0\n",
      "     BatchNorm1d-115               [-1, 64, 93]             128\n",
      "       Dropout1d-116               [-1, 64, 93]               0\n",
      "   ConstantPad1d-117              [-1, 64, 108]               0\n",
      "          Conv1d-118               [-1, 64, 93]          65,600\n",
      "     BatchNorm1d-119               [-1, 64, 93]             128\n",
      "       Dropout1d-120               [-1, 64, 93]               0\n",
      "          Conv1d-121               [-1, 64, 46]          65,600\n",
      "       MaxPool1d-122               [-1, 64, 46]               0\n",
      "RepeatedResidualBlock-123               [-1, 64, 46]               0\n",
      "     BatchNorm1d-124               [-1, 64, 46]             128\n",
      "       Dropout1d-125               [-1, 64, 46]               0\n",
      "   ConstantPad1d-126               [-1, 64, 61]               0\n",
      "          Conv1d-127               [-1, 64, 46]          65,600\n",
      "     BatchNorm1d-128               [-1, 64, 46]             128\n",
      "       Dropout1d-129               [-1, 64, 46]               0\n",
      "   ConstantPad1d-130               [-1, 64, 61]               0\n",
      "          Conv1d-131               [-1, 64, 46]          65,600\n",
      "       MaxPool1d-132               [-1, 64, 46]               0\n",
      "RepeatedResidualBlock-133               [-1, 64, 46]               0\n",
      "     BatchNorm1d-134               [-1, 64, 46]             128\n",
      "       Dropout1d-135               [-1, 64, 46]               0\n",
      "   ConstantPad1d-136               [-1, 64, 61]               0\n",
      "          Conv1d-137               [-1, 64, 46]          65,600\n",
      "     BatchNorm1d-138               [-1, 64, 46]             128\n",
      "       Dropout1d-139               [-1, 64, 46]               0\n",
      "          Conv1d-140               [-1, 64, 23]          65,600\n",
      "       MaxPool1d-141               [-1, 64, 23]               0\n",
      "RepeatedResidualBlock-142               [-1, 64, 23]               0\n",
      "     BatchNorm1d-143               [-1, 64, 23]             128\n",
      "       Dropout1d-144               [-1, 64, 23]               0\n",
      "   ConstantPad1d-145               [-1, 64, 38]               0\n",
      "          Conv1d-146               [-1, 64, 23]          65,600\n",
      "     BatchNorm1d-147               [-1, 64, 23]             128\n",
      "       Dropout1d-148               [-1, 64, 23]               0\n",
      "   ConstantPad1d-149               [-1, 64, 38]               0\n",
      "          Conv1d-150               [-1, 64, 23]          65,600\n",
      "       MaxPool1d-151               [-1, 64, 23]               0\n",
      "RepeatedResidualBlock-152               [-1, 64, 23]               0\n",
      "     BatchNorm1d-153               [-1, 64, 23]             128\n",
      "          Linear-154                   [-1, 14]          20,622\n",
      "================================================================\n",
      "Total params: 2,125,134\n",
      "Trainable params: 2,125,134\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 72.94\n",
      "Params size (MB): 8.11\n",
      "Estimated Total Size (MB): 81.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 6000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff1d75",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "**NETWORK FOR HUMAN ACTION CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ce69b",
   "metadata": {},
   "source": [
    "mplement a two stream network for human action classification as proposed by\n",
    "Simonyan and Zisserman in Two-Stream Convolutional Networks for Action\n",
    "Recognition in Videos2. The task consists in classifying the frames in a video\n",
    "according to the action performed by the human. This particular architectures\n",
    "operates on a frame by frame basis.\n",
    "Assuming that:\n",
    "- The network takes as input one single frame and its corresponding optical\n",
    "flow\n",
    "o The optical flow3 is already calculated and provided as additional\n",
    "input\n",
    "o The input size of both RGB and optical flow are 224 x 224\n",
    "o The optical flow for 2*L consecutive frames are encoded as 2*L\n",
    "grayscale input channels to the temporal stream branch\n",
    "- The two branches have the same architecture but separate (not shared)\n",
    "parameters\n",
    "- The number of possible actions is 100\n",
    "- The class score fusion consists in taking the average of the two predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c5fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "NUM_CLASSES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5be813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialStreamConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialStreamConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 96, 7, stride=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 256, 5, stride=2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3)\n",
    "\n",
    "        self.conv4_5 = nn.Conv2d(512, 512, 3)\n",
    "\n",
    "        self.fcl1 = nn.Linear(3*3*512, 4096)\n",
    "\n",
    "        self.dp = nn.Dropout2d(0.7)\n",
    "\n",
    "        self.fcl2 = nn.Linear(4096, 2048)\n",
    "\n",
    "        self.fcl3 = nn.Linear(2048, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = F.relu(self.conv4_5(x))\n",
    "\n",
    "        x = self.pool(F.relu(self.conv4_5(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fcl1(x))\n",
    "\n",
    "        x = F.relu(self.fcl2(x))\n",
    "\n",
    "        x = self.fcl3(x)\n",
    "\n",
    "        logits = F.softmax(x, dim=1)\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e6735ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalStreamConvNet(nn.Module):\n",
    "    def __init__(self, l):\n",
    "        super(TemporalStreamConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2*l, 96, 7, stride=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 256, 5, stride=2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3)\n",
    "\n",
    "        self.conv4_5 = nn.Conv2d(512, 512, 3)\n",
    "\n",
    "        self.fcl1 = nn.Linear(3*3*512, 4096)\n",
    "\n",
    "        self.dp = nn.Dropout2d(0.7)\n",
    "\n",
    "        self.fcl2 = nn.Linear(4096, 2048)\n",
    "\n",
    "        self.fcl3 = nn.Linear(2048, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = F.relu(self.conv4_5(x))\n",
    "\n",
    "        x = self.pool(F.relu(self.conv4_5(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fcl1(x))\n",
    "\n",
    "        x = F.relu(self.fcl2(x))\n",
    "\n",
    "        x = self.fcl3(x)\n",
    "\n",
    "        logits = F.softmax(x, dim=1)\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b96cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanActionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HumanActionNet, self).__init__()\n",
    "        self.SpatialNet = SpatialStreamConvNet()\n",
    "        self.TemporalNet = TemporalStreamConvNet(L)\n",
    "    \n",
    "    def forward(self, frame, optical_flow):\n",
    "        spatial_score = self.SpatialNet(frame)\n",
    "        temporal_score = self.TemporalNet(optical_flow)\n",
    "\n",
    "        avg = (spatial_score + temporal_score) / 2\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58634f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "frame = torch.randn(1, 3, 224, 224)\n",
    "optical_flow = torch.randn(1, 2*L, 224, 224)\n",
    "\n",
    "model = HumanActionNet()\n",
    "\n",
    "out = model(frame, optical_flow)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c97d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 96, 109, 109]          14,208\n",
      "       BatchNorm2d-2         [-1, 96, 109, 109]             192\n",
      "         MaxPool2d-3           [-1, 96, 54, 54]               0\n",
      "            Conv2d-4          [-1, 256, 25, 25]         614,656\n",
      "       BatchNorm2d-5          [-1, 256, 25, 25]             512\n",
      "         MaxPool2d-6          [-1, 256, 12, 12]               0\n",
      "            Conv2d-7          [-1, 512, 10, 10]       1,180,160\n",
      "            Conv2d-8            [-1, 512, 8, 8]       2,359,808\n",
      "            Conv2d-9            [-1, 512, 6, 6]       2,359,808\n",
      "        MaxPool2d-10            [-1, 512, 3, 3]               0\n",
      "           Linear-11                 [-1, 4096]      18,878,464\n",
      "           Linear-12                 [-1, 2048]       8,390,656\n",
      "           Linear-13                  [-1, 100]         204,900\n",
      "SpatialStreamConvNet-14                  [-1, 100]               0\n",
      "           Conv2d-15         [-1, 96, 109, 109]          94,176\n",
      "      BatchNorm2d-16         [-1, 96, 109, 109]             192\n",
      "        MaxPool2d-17           [-1, 96, 54, 54]               0\n",
      "           Conv2d-18          [-1, 256, 25, 25]         614,656\n",
      "      BatchNorm2d-19          [-1, 256, 25, 25]             512\n",
      "        MaxPool2d-20          [-1, 256, 12, 12]               0\n",
      "           Conv2d-21          [-1, 512, 10, 10]       1,180,160\n",
      "           Conv2d-22            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-23            [-1, 512, 6, 6]       2,359,808\n",
      "        MaxPool2d-24            [-1, 512, 3, 3]               0\n",
      "           Linear-25                 [-1, 4096]      18,878,464\n",
      "           Linear-26                 [-1, 2048]       8,390,656\n",
      "           Linear-27                  [-1, 100]         204,900\n",
      "TemporalStreamConvNet-28                  [-1, 100]               0\n",
      "================================================================\n",
      "Total params: 68,086,696\n",
      "Trainable params: 68,086,696\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 576240.00\n",
      "Forward/backward pass size (MB): 46.25\n",
      "Params size (MB): 259.73\n",
      "Estimated Total Size (MB): 576545.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, [(3, 224, 224), (2*L, 224, 224)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40cb781",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "**HOUSE PRICE PREDICTOR**\n",
    "Let us define a neural network to predict the price of the house starting from\n",
    "- One or more images of the interior and exterior\n",
    "- A set of categorical data\n",
    "In particular, refer to the dataset described here to retrieve the list of input\n",
    "variables https://github.com/emanhamed/Houses-dataset\n",
    "The high level architecture is depicted in the figure below. For extracting features\n",
    "from the image, use a CNN of your choice pre-trained on ImageNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85437e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PER_HOUSE = 4\n",
    "N_SAMPLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cd8a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "import torchvision.models as md\n",
    "resnet = md.resnet50(weights=md.ResNet50_Weights.DEFAULT)\n",
    "resnet.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fe6f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fcl1 = nn.Linear(in_features, 128)\n",
    "        self.fcl2 = nn.Linear(128, 128)\n",
    "        self.fcl3 = nn.Linear(128, 64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fcl2(F.sigmoid(self.fcl1(x))))\n",
    "\n",
    "        out = F.sigmoid(self.fcl3(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2d5d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HousePredictor, self).__init__()\n",
    "        self.cnn = resnet\n",
    "        self.mlp = MLP(4)\n",
    "        self.fcl = nn.Linear(2112, 256)\n",
    "        self.head = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, textual_data, images):\n",
    "        if textual_data.dim() == 1:\n",
    "            textual_data = textual_data.unsqueeze(0) #nneded for nn.Linear that expects (batch_size, n_features)\n",
    "\n",
    "        \n",
    "        if images.dim()==5: #handling batch of samples (houses) --> 5D tensors\n",
    "            batch_size, num_images, C, H, W = images.shape\n",
    "            images = images.view(batch_size * num_images, C, H, W) #if batch_size = 2 --> (8, 3, 224, 224)\n",
    "            cnn_out = self.cnn(images) #(8, 2048)\n",
    "            cnn_out = cnn_out.view(batch_size, num_images, -1) #(2, 4, 2048)\n",
    "            cnn_feat = cnn_out.mean(dim=1)  # (2, 2048)\n",
    "        else:\n",
    "            cnn_out = self.cnn(images) #gives features for each image in the batch (4 images)\n",
    "            cnn_feat = cnn_out.mean(dim=0, keepdim=True) #batch mean preserving dimension (1, 2048)\n",
    "            \n",
    "        mlp_out = self.mlp(textual_data) #(1, 64) if single batch and (2, 64) if 2 batches\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        conc = torch.cat([mlp_out, cnn_feat], dim=1)\n",
    "        x = F.relu(self.fcl(conc))\n",
    "        out = self.head(x)\n",
    "        return out #(1, 1)  or (2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d3b9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "images = torch.randn(N_SAMPLES, IMAGES_PER_HOUSE, 3, 224, 224) #ImageNet type\n",
    "\n",
    "df = pd.read_csv('./data/data.csv')\n",
    "categorical_colums = list(df.select_dtypes(['object']).columns)\n",
    "\n",
    "if len(categorical_colums) != 0:\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_colums:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "train_cols = [col for col in df.columns if col!='Price' and col != 'Nmb']\n",
    "\n",
    "textual_data = df[train_cols].values.astype(np.float32)\n",
    "textual_data = torch.from_numpy(textual_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0ef97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 3, 224, 224])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(textual_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba037650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "model = HousePredictor()\n",
    "for sample_images, sample_textual_data in zip(images, textual_data):\n",
    "    out = model(sample_textual_data, sample_images)\n",
    "\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35c0a3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "HousePredictor                                [1, 1]                    --\n",
       "├─ResNet: 1-1                                 [4, 2048]                 --\n",
       "│    └─Conv2d: 2-1                            [4, 64, 112, 112]         9,408\n",
       "│    └─BatchNorm2d: 2-2                       [4, 64, 112, 112]         128\n",
       "│    └─ReLU: 2-3                              [4, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-4                         [4, 64, 56, 56]           --\n",
       "│    └─Sequential: 2-5                        [4, 256, 56, 56]          --\n",
       "│    │    └─Bottleneck: 3-1                   [4, 256, 56, 56]          75,008\n",
       "│    │    └─Bottleneck: 3-2                   [4, 256, 56, 56]          70,400\n",
       "│    │    └─Bottleneck: 3-3                   [4, 256, 56, 56]          70,400\n",
       "│    └─Sequential: 2-6                        [4, 512, 28, 28]          --\n",
       "│    │    └─Bottleneck: 3-4                   [4, 512, 28, 28]          379,392\n",
       "│    │    └─Bottleneck: 3-5                   [4, 512, 28, 28]          280,064\n",
       "│    │    └─Bottleneck: 3-6                   [4, 512, 28, 28]          280,064\n",
       "│    │    └─Bottleneck: 3-7                   [4, 512, 28, 28]          280,064\n",
       "│    └─Sequential: 2-7                        [4, 1024, 14, 14]         --\n",
       "│    │    └─Bottleneck: 3-8                   [4, 1024, 14, 14]         1,512,448\n",
       "│    │    └─Bottleneck: 3-9                   [4, 1024, 14, 14]         1,117,184\n",
       "│    │    └─Bottleneck: 3-10                  [4, 1024, 14, 14]         1,117,184\n",
       "│    │    └─Bottleneck: 3-11                  [4, 1024, 14, 14]         1,117,184\n",
       "│    │    └─Bottleneck: 3-12                  [4, 1024, 14, 14]         1,117,184\n",
       "│    │    └─Bottleneck: 3-13                  [4, 1024, 14, 14]         1,117,184\n",
       "│    └─Sequential: 2-8                        [4, 2048, 7, 7]           --\n",
       "│    │    └─Bottleneck: 3-14                  [4, 2048, 7, 7]           6,039,552\n",
       "│    │    └─Bottleneck: 3-15                  [4, 2048, 7, 7]           4,462,592\n",
       "│    │    └─Bottleneck: 3-16                  [4, 2048, 7, 7]           4,462,592\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [4, 2048, 1, 1]           --\n",
       "│    └─Identity: 2-10                         [4, 2048]                 --\n",
       "├─MLP: 1-2                                    [1, 64]                   --\n",
       "│    └─Linear: 2-11                           [1, 128]                  640\n",
       "│    └─Linear: 2-12                           [1, 128]                  16,512\n",
       "│    └─Linear: 2-13                           [1, 64]                   8,256\n",
       "├─Linear: 1-3                                 [1, 256]                  540,928\n",
       "├─Linear: 1-4                                 [1, 1]                    257\n",
       "===============================================================================================\n",
       "Total params: 24,074,625\n",
       "Trainable params: 24,074,625\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 16.35\n",
       "===============================================================================================\n",
       "Input size (MB): 2.41\n",
       "Forward/backward pass size (MB): 711.30\n",
       "Params size (MB): 96.30\n",
       "Estimated Total Size (MB): 810.01\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, [(4, ), (4, 3, 224, 224)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151f022",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "**SIAMESE NETWORK**\n",
    "1) Implement a simple Siamese network for face verification. The Siamese\n",
    "network must\n",
    "• take as input two images\n",
    "• convert them into a vector of fixed length using a set of convolutional and\n",
    "dense layers (hint: remember that both stream need to share the weights!)\n",
    "• compute the distance\n",
    "2) At inference time, only the part of the model that computes f(x) is needed.\n",
    "Extract the subnetwork from the Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "87fbe890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreNetwork(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CoreNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool2_3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fcl1 = nn.Linear(55*55*256, 128)\n",
    "        self.dp = nn.Dropout2d(0.7)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        x = self.dp(x)\n",
    "\n",
    "        x = self.pool2_3(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = self.dp(x)\n",
    "\n",
    "        x = self.pool2_3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        x = self.dp(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        out = F.relu(self.fcl1(x))\n",
    "\n",
    "        return out #return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6812d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.core = CoreNetwork(in_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        f_x1 = self.core(x1)\n",
    "        f_x2 = self.core(x2)\n",
    "\n",
    "        diff = torch.abs(f_x1 - f_x2).sum(dim=1) #compute L1 distance in a batch\n",
    "\n",
    "        return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3647a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([63.2710, 54.4061, 57.1568, 54.2596, 66.8101, 59.6038, 75.0633, 49.2480,\n",
       "        69.4429, 87.5928], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Siamese(3)\n",
    "\n",
    "#2 batches of 10 images, the pair \"i\" of images is composed by the image at position \"i\" in both batches\n",
    "images1 = torch.randn(10, 3, 224, 224) #batch of 10 images for siamese network\n",
    "images2 = torch.randn(10, 3, 224, 224) #batch of 10 images for siamese network\n",
    "\n",
    "out = model(images1, images2) #we will have 10 values --> (10,)\n",
    "\n",
    "print(out.shape)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ff4b9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "         MaxPool2d-3         [-1, 64, 224, 224]               0\n",
      "         Dropout2d-4         [-1, 64, 224, 224]               0\n",
      "            Conv2d-5        [-1, 128, 220, 220]         204,928\n",
      "       BatchNorm2d-6        [-1, 128, 220, 220]             256\n",
      "         MaxPool2d-7        [-1, 128, 110, 110]               0\n",
      "         Dropout2d-8        [-1, 128, 110, 110]               0\n",
      "            Conv2d-9        [-1, 256, 110, 110]         295,168\n",
      "      BatchNorm2d-10        [-1, 256, 110, 110]             512\n",
      "        MaxPool2d-11          [-1, 256, 55, 55]               0\n",
      "        Dropout2d-12          [-1, 256, 55, 55]               0\n",
      "           Linear-13                  [-1, 128]      99,123,328\n",
      "      CoreNetwork-14                  [-1, 128]               0\n",
      "           Conv2d-15         [-1, 64, 224, 224]           1,792\n",
      "      BatchNorm2d-16         [-1, 64, 224, 224]             128\n",
      "        MaxPool2d-17         [-1, 64, 224, 224]               0\n",
      "        Dropout2d-18         [-1, 64, 224, 224]               0\n",
      "           Conv2d-19        [-1, 128, 220, 220]         204,928\n",
      "      BatchNorm2d-20        [-1, 128, 220, 220]             256\n",
      "        MaxPool2d-21        [-1, 128, 110, 110]               0\n",
      "        Dropout2d-22        [-1, 128, 110, 110]               0\n",
      "           Conv2d-23        [-1, 256, 110, 110]         295,168\n",
      "      BatchNorm2d-24        [-1, 256, 110, 110]             512\n",
      "        MaxPool2d-25          [-1, 256, 55, 55]               0\n",
      "        Dropout2d-26          [-1, 256, 55, 55]               0\n",
      "           Linear-27                  [-1, 128]      99,123,328\n",
      "      CoreNetwork-28                  [-1, 128]               0\n",
      "================================================================\n",
      "Total params: 199,252,224\n",
      "Trainable params: 199,252,224\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 86436.00\n",
      "Forward/backward pass size (MB): 550.50\n",
      "Params size (MB): 760.09\n",
      "Estimated Total Size (MB): 87746.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, [(3, 224, 224), (3, 224, 224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "83eea5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoreNetwork(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fcl1): Linear(in_features=774400, out_features=128, bias=True)\n",
       "  (dp): Dropout2d(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_x = model.core #to use at inference time\n",
    "f_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
