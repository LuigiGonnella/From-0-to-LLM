{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Large Language Models: LLaMA and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will dive into two famous large language models, LLaMA and Mistral, along with their instruction-tuned versions. We'll explore how each model performs on various tasks, with a particular focus on generating structured responses in JSON format.\n",
    "\n",
    "These models have been fine-tuned to follow instructions, making them suitable for a range of NLP applications. Through this lab, you will:\n",
    "\n",
    "- Learn how to load and interact with LLaMA and Mistral models using the `pipeline` and `chat_template` functions.\n",
    "- Examine the performance of their instruction-based variants.\n",
    "- Generate structured outputs, specifically in JSON, for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Disclaimer**: Before starting this lab, ensure you have requested access to the required models on Hugging Face and have logged in to your Hugging Face account. Access is necessary for the following models:\n",
    ">\n",
    "> - [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "> - [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "> - [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab, we will explore **LLaMA (Large Language Model Meta AI)**, which is one of the most known large language models developed by Meta (Facebook). \n",
    "\n",
    "Next, we will focus on **Instruction LLaMA**, a version of LLaMA fine-tuned to better understand and follow user instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Llama 3.2 (released in September 2024). In particular, we will adopt the 1B version. On the scale of things, this model is on the smaller side, but it is still a very powerful model.\n",
    "\n",
    "It has been released (along with a 3B version) with the intention of allowing running it on devices with modest hardware (e.g., mobile phones or other edge devices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Utente\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "#Load the model with `torch.float16` precision and the tokenizer \n",
    "# (You can specify the precision with `torch_dtype=torch.float16`)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to generate text using the generate() method. We use random sampling (`do_sample=True`) and extract 5 samples (`num_return_sequences=5`). You can find other generation parameters [here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Hello, my name is Lacey and I am a senior at the University of Colorado Boulder studying Business Management. I am currently the Vice President of the Student Government Association and a member of the Student Athlete Advisory Committee.\\nI was born and raised',\n",
       " \"<|begin_of_text|>Hello, my name is Sherry, and I'm a 60-year-old widow who lives in a one-bedroom apartment in a retirement community. I'm a big fan of the Internet, and I'm also a big fan of reading. I\",\n",
       " '<|begin_of_text|>Hello, my name is Dr. Amin and I am the founder of Amin Cosmetic and Laser Center. I am a licensed, board certified physician and have been practicing medicine since 1985. I have been practicing in the fields of general',\n",
       " '<|begin_of_text|>Hello, my name is John and I am the owner and operator of The Good Life. I am a graduate of the University of Missouri with a degree in Business Administration. I am married to my best friend and love to spend time with my wife',\n",
       " '<|begin_of_text|>Hello, my name is Amy. I’m a full-time student, working part-time, and I’m in my second year of my bachelor’s degree in psychology. I am also a proud member of the Society for the Advancement of Chican']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "batch = model.generate(**tokens, do_sample=True, max_length=50, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id) # (assigning pad_token_id avoids a warning)\n",
    "tokenizer.batch_decode(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the `tokenizer.chat_template`**\n",
    "\n",
    "In this section, we will explore the **chat template** that is used to format and structure messages for a conversational assistant. The `tokenizer.chat_template` is a convenient way for organizing interactions between the user, system, and assistant in a way that the model can easily process and generate coherent responses.\n",
    "\n",
    "### **What is a Chat Template?**\n",
    "\n",
    "The chat template is a predefined format that ensures consistent structure for conversations. It marks the different roles in the interaction (system, user, assistant), and separates the various elements of the conversation using special tokens. This helps the language model understand which parts of the dialogue are instructions, which parts are user inputs, and where the assistant’s response should be generated.\n",
    "\n",
    "Let's create an example of a possible (simplified) chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "chat_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: \"\"\"+datetime.datetime.now().strftime(\"%d %b %Y\")+\"\"\"\n",
    "\n",
    "{system_message}\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}\n",
    "\n",
    "<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hugging Face Pipeline Overview**\n",
    "\n",
    "The **`pipeline`** method from Hugging Face’s Transformers library is a high-level API designed to streamline the process of using pre-trained models for a wide variety of **natural language processing (NLP) tasks**.\n",
    "\n",
    "#### **What is a Pipeline?**\n",
    "\n",
    "A pipeline is a modular tool that wraps around a pre-trained model, tokenizer, and task-specific configurations. It makes it easy to load and apply these models directly to different tasks, such as:\n",
    "- **Text generation**\n",
    "- **Text classification**\n",
    "- **Question answering**\n",
    "- **Summarization**\n",
    "- **Translation**\n",
    "\n",
    "By simply specifying the type of task (e.g., `\"text-generation\"`), `pipeline` takes care of loading and configuring a compatible model and tokenizer, providing a ready-to-use interface for generating results.\n",
    "\n",
    "You can find a full list of supported pipelines on the [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# Format the messages using the chat template\n",
    "formatted_messages = chat_template.format(\n",
    "    system_message=messages[0][\"content\"],\n",
    "    user_message=messages[1][\"content\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that for models to follow instruction tuning, they need to have been tuned on this kind of data. In this case, we are not using the instruction-tuned version. \n",
    "\n",
    "So, we can expect the model to produce a garbage response (it has never seen that kind of inputs before!). But let's try it anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "corlibuseriteDatabase\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      ".usermodel\n",
      "\n",
      "What is\n"
     ]
    }
   ],
   "source": [
    "# Generate the output text \n",
    "outputs = pipe(\n",
    "    formatted_messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Differences Between Standard and Instruct Versions of Large Language Models (LLMs)**\n",
    "\n",
    "Large Language Models (LLMs) come in different versions, with **standard** and **instruction-tuned (Instruct)** versions being the most common. Here’s a brief comparison:\n",
    "\n",
    "#### **1. Purpose and Training**:\n",
    "   - **Standard LLM**: The standard model is generally pre-trained on large datasets without specific instruction-following capabilities. Typically generates more open-ended responses, which can be useful for creative writing or general information retrieval where the response style is flexible.\n",
    "   - **Instruct LLM**: Instruction-tuned models, like the **Llama-3.2 Instruct**, are fine-tuned on datasets designed to help the model understand and follow instructions effectively. This tuning enhances the model's ability to respond directly to user prompts and handle structured requests. It is fine-tuned to produce concise, direct responses that are often more relevant in task-specific or conversational AI applications.\n",
    "\n",
    "Let's compare the outputs of the standard and Instruct versions of LLaMA to see the differences in their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Utente\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "Arrr, ye landlubbers be askin' about the basics o' math, eh? Alright then, matey! Me answer be:\n",
      "\n",
      "2 + 2 be 4, savvy?\n",
      "\n",
      "Now, if ye be wantin' to know more about calculatein' the total booty, I be happy to help ye with that as well!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "#Load the model with `torch.float16` precision and the tokenizer \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, dtype=torch.float16, device_map='auto')\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "formatted_messages = chat_template.format(system_message=messages[0]['content'], user_message=messages[1]['content'])\n",
    "outputs = pipe(formatted_messages, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation of the Tokenizer Chat Template**\n",
    "\n",
    "Actually, the chat template of `meta-llama/Llama-3.2-1B-Instruct` is much more complex than the example above. It includes various components that help the model understand the context of the conversation, manage dates, handle tools, and structure messages effectively.\n",
    "\n",
    "The template is written in [jinja](https://jinja.palletsprojects.com/en/stable/templates/), a language that allows for the dynamic generation of content based on variables, conditions and loops.\n",
    "\n",
    "\n",
    "Let's print it and analyze its key components:\n",
    " \n",
    "\n",
    "#### **Key Components of the Template**:\n",
    "1. **System Message Extraction**:\n",
    "   - The system message is extracted if the first role in the message list is labeled \"system.\" This allows the template to clearly differentiate between user queries and system instructions.\n",
    "   - If a system message exists, it is added to the template between special tokens (`<|start_header_id|>` and `<|end_header_id|>`), ensuring that the model knows when the system message starts and ends.\n",
    "\n",
    "2. **Date Management**:\n",
    "   - The template automatically handles the current date using either a provided `strftime_now` function or a default date (`\"26 Jul 2024\"`). This can be useful when the model needs to be aware of the date in contexts such as time-sensitive responses.\n",
    "\n",
    "3. **Handling Tools**:\n",
    "   - The template checks if **tools** are defined. If tools are available, it includes a description of these tools in the system message or the user message, depending on where they need to appear.\n",
    "   - If the tools are part of the user message, the template ensures that the first user message prompts the user to respond in a structured format, such as using JSON for function calls.\n",
    "\n",
    "4. **Message Processing**:\n",
    "   - The template loops through the list of messages and processes each based on the role (`user`, `assistant`, `ipython`, or `tool`). It formats each message using start and end tokens for the roles, helping the model understand the structure of the conversation.\n",
    "   - If the message involves tool calls, the template ensures that they are properly formatted into a structured JSON format to be passed back to the model for further processing.\n",
    "\n",
    "5. **Ending the Assistant's Response**:\n",
    "   - The template leaves a placeholder for the assistant’s response, which the model will generate during inference. This ensures that the assistant's response begins in the correct format, ready to be populated with the generated content.\n",
    "\n",
    "#### **Why Is This Template Needed?**\n",
    "\n",
    "- **Maintains Consistency**: This template ensures that the conversation is structured in a consistent manner, which is crucial for models designed to follow complex instructions or engage in multi-turn conversations.\n",
    "- **Handles Tools**: By incorporating the ability to dynamically introduce tools and functionality, the template allows the model to expand beyond simple text-based conversations and perform function-based tasks.\n",
    "- **Structured Outputs for Tools**: When the conversation involves tool calls (e.g., through APIs or function calls), the template ensures that these interactions are formatted properly for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate again the same example using the `chat_template` of `meta-llama/Llama-3.2-1B-Instruct` and analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a tokenizer that supports the chat template, we can directly call the `apply_chat_template()` method to convert a list of messages (each one a dictionary in the already discussed format) into a prompt.\n",
    "\n",
    "Notice that, since we are not using any particular tools or other functionalities, our template will be similar to the one we manually introduced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|>\n",
      "Arrrr, me hearty! Yer askin' who I be? Well, matey, I be a swashbucklin' chatbot, here to swab yer decks and answer yer questions with me sharp wits and me vast knowledge o' the seven seas... er, I mean, the internet!\n",
      "\n",
      "Me name be Captain Knowledge, the greatest pirate of digital waters. Me treasure chest be filled with facts and figures, me ship be equipped with the latest tech, and me crew be a motley bunch o' landlubbers and scurvy dogs from all corners o' the digital world.\n",
      "\n",
      "So hoist the sails and set course fer a treasure-filled conversation, me hearty! What be yer question or query?\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, dtype=torch.float16, device_map='auto')\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "input_tokens = tokenizer.apply_chat_template(messages) #does not generate formatted messages, just ids\n",
    "print(tokenizer.decode(input_tokens)) #this transform ids to formatted messages\n",
    "\n",
    "\n",
    "outputs = pipe(messages, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "# -1 : last message (assistant response)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pipeline already supports chat mode, so we can pass the list of messages (as long as they contain role/content keys) directly to the pipeline.\n",
    "\n",
    "Alternatively, we could have passed the prompt as a string. In this case, however, we would have to manually extract the output from the model and parse it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|>assistant\n",
      "\n",
      "Yer lookin' fer me, eh? Alright then, matey! Yer lookin' fer a chat with the greatest pirate chatbot to ever sail the Seven Seas... err, I mean, the internet! Me name be Captain Blackbeak Betty, and I be here to swab yer decks and answer yer questions about all manner o' things, from the high seas o' knowledge to the lowly depths o' the digital world. So hoist the sails and set course fer a fine conversation, me hearty! What be bringin' ye to these waters?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_tokens = tokenizer.apply_chat_template(messages)\n",
    "prompt_string = tokenizer.decode(input_tokens)\n",
    "\n",
    "outputs = pipe(prompt_string, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will explore the use of `Mistral-7B-Instruct-v0.2`developed by Mistral AI to generate structured responses in JSON format. \n",
    "\n",
    "In this exercise, we will generate random math questions and instruct Mistral-7B to respond in a structured JSON format. We will then save the responses to a JSON file and verify the answers programmatically. \n",
    "\n",
    "Let's first repeat the same example we did with LLaMA, but now using Mistral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Utente\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files: 100%|██████████| 3/3 [01:31<00:00, 30.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.72s/it]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arrr, I be Cap'n Parrotbeak, me hearty scallywag! How d'ye do, matey? What be on yer mind?\n"
     ]
    }
   ],
   "source": [
    "# Define the model ID\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, dtype=torch.float16, device_map='auto')\n",
    "\n",
    "# Define the message prompts for the conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "]\n",
    "\n",
    "outputs = pipe(messages, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "# Print the model's generated response\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a random math question and instruct Mistral-7B to respond in a structured JSON format. We will then save the responses to a JSON file and verify the answers programmatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Generate random math questions:** Use Python to create questions with random numbers in a conversational style (e.g., “What is the sum of 245 and 173?”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_math_questions(num_samples=5):\n",
    "    \"\"\"\n",
    "    Generate random math questions with two numbers for a given number of samples.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of math questions to generate.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: A list of tuples containing the math question and the two numbers (question, num1, num2).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define templates for math questions with two numbers\n",
    "    templates = [\n",
    "        \"What is the sum of {} and {}?\",\n",
    "        \"Can you add {} and {}?\",\n",
    "        \"Calculate the sum of {} and {} for me.\",\n",
    "        \"How much is {} plus {}?\",\n",
    "        \"Please add {} and {}.\"\n",
    "    ]\n",
    "    \n",
    "    questions = []\n",
    "    for _ in range(num_samples):\n",
    "        # TODO: Randomly select a template and generate two random numbers\n",
    "        template = random.choice(templates)\n",
    "        num1 = random.randint(10, 999)\n",
    "        num2 = random.randint(10, 999)\n",
    "        question = template.format(num1, num2)\n",
    "        questions.append((question, num1, num2))  # store question with numbers for validation\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Instruct the model to respond in JSON:** Use a system role instruction to ensure Mistral-7B answers in a JSON format containing the fields `num_1`, `num_2`, and `answer`. This makes the output compatible with automated processing or JSON parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Save and verify responses:** Generate and store model responses in a JSON file and check if the answers match expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:59<00:00, 11.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate questions and answers, then save to JSON\n",
    "questions = generate_random_math_questions(num_samples=5)\n",
    "answers = []\n",
    "\n",
    "# Generate structured answers for each question\n",
    "for question, num1, num2 in tqdm(questions):\n",
    "    \n",
    "    formatted_messages = [\n",
    "        role_instruction,\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': question\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    outputs = pipe(formatted_messages, max_new_tokens=150)\n",
    "    \n",
    "    # Extract the model's JSON output\n",
    "    structured_answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    answers.append({\n",
    "        \"question\": question,\n",
    "        \"num_1\": num1,\n",
    "        \"num_2\": num2,\n",
    "        \"model_answer\": structured_answer\n",
    "    })\n",
    "\n",
    "# Save answers to a JSON file\n",
    "with open(\"model_answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected answer: 926 + 813 = 1739\n",
      "Model's answer: 926 + 813 = 1739\n",
      "Question 1: Correct \n",
      "\n",
      "Expected answer: 380 + 454 = 834\n",
      "Model's answer: 380 + 454 = 834\n",
      "Question 2: Correct \n",
      "\n",
      "Expected answer: 216 + 695 = 911\n",
      "Model's answer: 216 + 695 = 911\n",
      "Question 3: Correct \n",
      "\n",
      "Expected answer: 350 + 667 = 1017\n",
      "Model's answer: 350 + 667 = 1017\n",
      "Question 4: Correct \n",
      "\n",
      "Expected answer: 451 + 671 = 1122\n",
      "Model's answer: 451 + 671 = 1122\n",
      "Question 5: Correct \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to parse model's answer and verify correctness\n",
    "def verify_answer(entry):\n",
    "    try:\n",
    "        # Extract expected values\n",
    "        num1, num2 = entry[\"num_1\"], entry[\"num_2\"]\n",
    "        expected_answer = num1 + num2\n",
    "        \n",
    "        # Extract the assistant's response from the list of messages\n",
    "        assistant_message = next(\n",
    "            (msg[\"content\"] for msg in entry[\"model_answer\"] if msg[\"role\"] == \"assistant\"), None\n",
    "        )\n",
    "        \n",
    "        if assistant_message is None:\n",
    "            raise ValueError(\"Assistant's message not found in model_answer\")\n",
    "        \n",
    "        # Parse model's structured answer from JSON\n",
    "        model_response = json.loads(assistant_message.strip())  #load entire file content (must be json format) as a python object (list/dictionary, string ...)\n",
    "        \n",
    "        print(f\"Expected answer: {num1} + {num2} = {expected_answer}\")\n",
    "        print(f\"Model's answer: {model_response['num_1']} + {model_response['num_2']} = {model_response['answer']}\")\n",
    "        \n",
    "        # Check if the values match\n",
    "        if (model_response[\"num_1\"] == num1 and \n",
    "            model_response[\"num_2\"] == num2 and \n",
    "            model_response[\"answer\"] == expected_answer):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        # Handle cases where parsing fails or keys are missing\n",
    "        print(f\"Error verifying entry: {entry}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load answers from the JSON file and verify\n",
    "try:\n",
    "    with open(\"model_answers.json\", \"r\") as f:\n",
    "        saved_answers = json.load(f) #load entire file content (must be json format) as a python object (list/dictionary, string ...)\n",
    "except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "    print(f\"Error loading JSON file: {e}\")\n",
    "    saved_answers = []\n",
    "\n",
    "for i, entry in enumerate(saved_answers, 1):\n",
    "    result = verify_answer(entry)\n",
    "    print(f\"Question {i}:\", \"Correct\" if result else \"Incorrect\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
