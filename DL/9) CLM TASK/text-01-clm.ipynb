{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Language Modeling (CLM) - Preprocess, Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore **Causal Language Modeling (CLM)**, which is a core task in training autoregressive language models like GPT-2. CLM is the process of predicting the next word in a sequence, given the previous words. This type of modeling forms the backbone of text generation tasks, where the model learns to generate coherent text by focusing only on previous tokens in the sequence.\n",
    "\n",
    "The lab is divided into three major sections:\n",
    "1. **Preprocessing**: Preparing a dataset and the labels for the CLM task and tokenizing them.\n",
    "2. **Training**: Fine-tuning a pre-trained language model like GPT-2 on a specific dataset using the CLM task.\n",
    "3. **Inference**: Evaluating the modelâ€™s performance by generating text based on input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the Domain-Specific Dataset**:\n",
    "   - The first step in fine-tuning GPT-2 is to load a dataset that is specific to the domain of interest. In this case, we are using a publicly available **medical dataset** from the **PubMed** collection. PubMed contains a vast number of medical articles, and fine-tuning on such a dataset can help GPT-2 generate more accurate and context-specific medical text.\n",
    "   - The dataset we're using is `\"japhba/pubmed_simple\"`, which is a simplified version of PubMed data. This dataset can be easily accessed using the `datasets` library from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)\n",
    "\n",
    "ds = load_dataset(\"japhba/pubmed_simple\", split=\"train\")\n",
    "train_dataset = ds.shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = ds.shuffle(seed=42).select(range(1000, 1500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the contents of any one of the examples in the dataset to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Purpose The identification of abnormalities that are relatively rare within otherwise normal anatomy is a major challenge for deep learning in the semantic segmentation of medical images. The small number of samples of the minority classes in the training data makes the learning of optimal classification challenging, while the more frequently occurring samples of the majority class hamper the generalization of the classification boundary between infrequently occurring target objects and classes. In this paper, we developed a novel generative multi-adversarial network, called Ensemble-GAN, for mitigating this class imbalance problem in the semantic segmentation of abdominal images.Method The Ensemble-GAN framework is composed of a single-generator and a multi-discriminator variant for handling the class imbalance problem to provide a better generalization than existing approaches. The ensemble model aggregates the estimates of multiple models by training from different initializations and losses from various subsets of the training data. The single generator network analyzes the input image as a condition to predict a corresponding semantic segmentation image by use of feedback from the ensemble of discriminator networks. To evaluate the framework, we trained our framework on two public datasets, with different imbalance ratios and imaging modalities: the Chaos 2019 and the LiTS 2017.Result In terms of the F1 score, the accuracies of the semantic segmentation of healthy spleen, liver, and left and right kidneys were 0.93, 0.96, 0.90 and 0.94, respectively. The overall F1 scores for simultaneous segmentation of the lesions and liver were 0.83 and 0.94, respectively.Conclusion The proposed Ensemble-GAN framework demonstrated outstanding performance in the semantic segmentation of medical images in comparison with other approaches on popular abdominal imaging benchmarks. The Ensemble-GAN has the potential to segment abdominal images more accurately than human experts.',\n",
       " 'country': 'US'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the each entry is a dictionary with two keys:\n",
    "- \"abstract\": The abstract of the medical article\n",
    "- \"country\": The country where the article was published (we will not use this information in this lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Tokenize the Dataset**:\n",
    "   - The next step is to **tokenize** the data so that it can be processed by GPT-2.\n",
    "   - GPT-2 does not natively use a padding token, since it does not require fixed length inputs. For this reason, we will substite it with the EOS token as suggested by transformers library (remember, we are also passing an attention mask, so whatever value is used for padding will be ignored by the model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples['abstract'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_eval = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Add labels to the Dataset for Next Token Prediction**:\n",
    "   - In this step, we will add the **labels** that will be used during the next token prediction task. \n",
    "   - In autoregressive language modeling, the **labels** represent the same sequence as the input, shifted one token to the right. This is because the model is trained to predict the next token in the sequence given the previous tokens.\n",
    "   - The shifting of the tokens is already handled automatically by the `Trainer` class. We just pass an extra attribute named `labels` to the dataset (when this argument is passed to the model, it will know to compute the loss for us!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(samples):\n",
    "    samples[\"labels\"] = samples['input_ids']\n",
    "    return samples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "tokenized_dataset_eval = tokenized_dataset_eval.map(add_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fine-Tune the GPT-2 Model**:\n",
    "   - Set up the model and finetune it using the medical dataset. \n",
    "   - The pipeline to be followed is the same that we have already seen in the previous lab (`lab03 - 01-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 1:17:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.733705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.718054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.717620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.716711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.716191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize GPT-2 Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Set Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medical-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=12,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "\n",
    "import transformers\n",
    "# Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_eval,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-medical-finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Compare Text Generation Before and After Fine-Tuning**:\n",
    "   - Generate text using both the original pre-trained GPT-2 model and the fine-tuned model.\n",
    "   - Provide the same input prompt and observe the differences in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained('./gpt2-medical-finetuned')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = \"The patient presents with chest pain and shortness of breath.\"\n",
    "\n",
    "inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Before Fine-Tuning (Pre-Trained GPT-2)**:\n",
      "The patient presents with chest pain and shortness of breath.\n",
      "\n",
      "Fearing for their life, they seek help from a relative or another physician who is able to handle the situation. If the emergency is too serious, they call 911.\n",
      "\n",
      "\"The doctor knows it won't be the last time she'll meet him,\" St. Patrick said.\n",
      "\n",
      "If the patient wishes to stay with their relatives, they can use the National Guard's Emergency Medical Services (EMSS) program.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_pretrained = pretrained_model.generate(input_ids, do_sample=True, max_length=100, temperature=1.0, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_pretrained = tokenizer.decode(output_pretrained[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"**Before Fine-Tuning (Pre-Trained GPT-2)**:\")\n",
    "print(generated_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**After Fine-Tuning (Fine-Tuned on Medical Dataset)**:\n",
      "The patient presents with chest pain and shortness of breath. It was found that all of the following symptoms affect the central nervous system of the patient, including anxiety, irritability, and agitation. Although this was not the final assessment, the patients did experience a wide range of side effects, and the severity of these side effects was consistent with a predisposition for pain-related diseases or conditions.\n"
     ]
    }
   ],
   "source": [
    "output_finetuned = finetuned_model.generate(input_ids, do_sample=True, max_length=100, temperature=1.0, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_finetuned = tokenizer.decode(output_finetuned[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"**After Fine-Tuning (Fine-Tuned on Medical Dataset)**:\")\n",
    "print(generated_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "Training the model in this way produces batches with potentially very different lengths. This can be inefficient, as the model will have to pad the sequences to the length of the longest sequence in the batch.\n",
    "\n",
    "To avoid this, we can use a technique called **Dynamic Padding**. This technique groups the sequences in the batch by length and pads them to the length of the longest sequence in each group. This way, the model only has to pad the sequences to the length of the longest sequence in each group, which can significantly reduce the amount of padding required.\n",
    "\n",
    "As a first exercise, quantify the number of pad tokens being used in various situations:\n",
    "1. You pad all batches to the maximum allowed sequence length (1024 for GPT-2, this is what we used so far)\n",
    "2. You pad the entire batch to the length of the longest sequence in the batch (generate the batches by randomly sampling sentences)\n",
    "3. You pad the entire batch to the length of the longest sequence in the batch (generate the batches by placing sentences of similar lengths together)\n",
    "\n",
    "Next, introduce dynamic padding and compare the execution times of the previous execution and the one with dynamic padding.\n",
    "\n",
    "You can use the following resources to help you with this exercise:\n",
    "- `group_by_length` parameter ([TrainingArguments](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments.group_by_length)) (parameter to group together samples with similar lengths)\n",
    "- `DataCollatorForSeq2Seq` ([DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)) (collator function that aggregates samples into batches and pads them to the maximum length of the batch)\n",
    "\n",
    "Note: you may find that the validation losses you observe may be different from the previous ones. This is because the cross entropy loss is computed as an average across tokens, and the number of tokens in a batch can vary depending on the padding strategy used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#CASE 1: we did it until now\n",
    "tot_tokens = sum((torch.tensor(x['input_ids'])==tokenizer.pad_token_id).sum().item() for x in tokenized_dataset)\n",
    "print(tot_tokens)\n",
    "\n",
    "#CASE 2: random sampling of sentences\n",
    "# Initialize GPT-2 Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples['abstract'], truncation=True) #no padding\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False #causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Set Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medical-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=6,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    group_by_length=False #random sampling of sentences\n",
    ")\n",
    "\n",
    "\n",
    "# Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_eval,\n",
    "    data_collator=data_collator #we add data_collator to pad up to maximum length niside the batch (instead of 1024)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-medical-finetuned-CASE2\")\n",
    "\n",
    "#CASE 3: sampling of sentences by length\n",
    "\n",
    "# Set Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medical-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=6,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    group_by_length=True #sampling by length\n",
    ")\n",
    "\n",
    "\n",
    "# Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_eval,\n",
    "    data_collator=data_collator #we add data_collator to pad up to maximum length niside the batch (instead of 1024)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-medical-finetuned-CASE3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
