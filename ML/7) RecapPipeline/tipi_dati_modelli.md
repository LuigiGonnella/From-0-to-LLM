# üìä Tabella: Tipi di dati ‚Üí Modelli consigliati

| Tipo di dati / Situazione | Modelli top consigliati | Perch√© / Punti di forza | Limitazioni & Attenzioni |
|---------------------------|-------------------------|--------------------------|---------------------------|
| **Dati numerici continui, target continuo (regressione)** | - Linear / Ridge / Lasso Regression<br>- Random Forest Regressor<br>- Gradient Boosting (XGBoost, LightGBM, CatBoost)<br>- Neural Networks (MLP) | - Regressioni lineari ottime per relazioni semplici e dataset piccoli<br>- RF/GBM robusti su dataset mediograndi, catturano non-linearit√†<br>- CatBoost gestisce anche categorici mixati<br>- NN potenti con dataset grandi e relazioni complesse | - Regressioni lineari richiedono scaling e feature engineering<br>- RF/GBM pi√π costosi su dataset enormi<br>- NN necessitano di molta data e tuning |
| **Dati tabellari misti (numerici + categorici)** | - CatBoost (miglior handling nativo dei categorici)<br>- LightGBM (supporto parziale ai categorici)<br>- Random Forest (dopo LabelEncoding, meglio evitare OHE se tante features)<br>- Logistic Regression (per baseline binaria) | - CatBoost elimina bisogno di heavy encoding<br>- RF/GBM robusti a rumore e non-linearit√†<br>- LR interpretabile come baseline | - Encoding richiesto per modelli lineari / SVM (OHE) <br>- RF/GBM meno interpretabili |
| **Target binario (classification)** | - Logistic Regression (baseline interpretabile)<br>- Random Forest / Gradient Boosting (alta accuracy)<br>- SVM (se dataset piccolo, margine chiaro)<br>- Neural Networks (se dati grandi e complessi) | - LR semplice, interpretabile<br>- RF/GBM top performer in tabular ML<br>- SVM ottimo per margini chiari<br>- NN flessibili e potenti | - LR limitata se relazione non lineare<br>- SVM non scalano bene con dataset enormi<br>- NN richiedono tuning e data preprocessing |
| **Target multiclasse (classification)** | - One-vs-Rest Logistic Regression / SVM<br>- Random Forest<br>- Gradient Boosting (CatBoost ottimo)<br>- Neural Networks (per dati complessi) | - RF/GBM gestiscono bene multiclass nativamente<br>- CatBoost ha handling avanzato multiclass<br>- NN adatti per immagini, testo, segnali | - Regressioni richiedono ovR/ovO strategy<br>- NN difficili da addestrare su dataset piccoli |
| **Dati categorici puri (senza numerici)** | - CatBoost (nativo)<br>- LightGBM (con encoding)<br>- Random Forest (dopo LabelEncoding, meglio evitare OHE se tante features)<br>- Naive Bayes (se dati testuali/categorici) | - CatBoost √® il gold standard<br>- NB molto veloce e semplice su testo | - One-hot pu√≤ esplodere dimensionalit√†<br>- NB fa assunzioni forti (indipendenza) |
| **Dati sbilanciati (es. frodi, rare events)** | - Logistic Regression + class weights<br>- Gradient Boosting con sampling/weights<br>- Random Forest con class weights<br>- Anomaly detection (Isolation Forest, One-Class SVM) | - Class weights o oversampling (SMOTE) funzionano bene<br>- Boosting pu√≤ gestire imbalance con tuning | - Accuracy fuorviante ‚Üí usare ROC-AUC, F1 |
| **Dataset piccoli (poche centinaia di campioni)** | - Logistic Regression<br>- SVM<br>- KNN<br>- Decision Trees | - SVM ottimo su pochi dati<br>- KNN semplice e non richiede training<br>- LR per interpretabilit√† | - KNN scala male su dataset grandi<br>- RF/GBM rischiano overfitting se dati scarsi |
| **Dataset enormi (milioni di righe)** | - Gradient Boosting (LightGBM, CatBoost)<br>- Online learning (SGDClassifier/Regressor)<br>- Neural Networks | - LightGBM ottimizzato per big data<br>- SGD scalabile e semplice | - RF scalano male su dataset giganteschi |
| **Testo / NLP** | - Naive Bayes (baseline veloce)<br>- Logistic Regression + TF-IDF<br>- Gradient Boosting su embedding<br>- Transformers (BERT, GPT-like) | - NB e LR ottimi baseline su bag-of-words<br>- Transformers SOTA su NLP complessi | - Transformers costosi e richiedono dataset grande |
| **Immagini** | - CNN (ResNet, EfficientNet)<br>- Vision Transformers | - CNN collaudate per vision<br>- ViT SOTA con grandi dataset | - Necessitano GPU e dataset enormi |
| **Serie temporali (forecasting)** | - ARIMA / SARIMA (baseline)<br>- Gradient Boosting con lag features<br>- RNN / LSTM / GRU<br>- Transformers (Time-series models) | - ARIMA interpretabile e semplice<br>- LSTM/Transformers gestiscono dipendenze lunghe | - ARIMA non scala bene con molte feature<br>- NN richiedono tuning e data scaling |
| **Clustering (non supervisionato)** | - KMeans (baseline)<br>- DBSCAN (cluster arbitrari, outlier detection)<br>- Gaussian Mixture Models<br>- Hierarchical Clustering | - KMeans ottimo su dati numerici densi<br>- DBSCAN robusto a outliers<br>- GMM probabilistico | - KMeans richiede scelta di K<br>- DBSCAN sensibile a iperparametri |
